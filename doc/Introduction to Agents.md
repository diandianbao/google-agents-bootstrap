# 智能体入门
作者：艾伦·布朗特、安东尼奥·古利、舒巴姆·萨布、迈克尔·齐默曼、弗拉基米尔·武斯科维奇  

谷歌公司  

## 智能体与智能体架构入门
### 致谢
- **内容贡献者**：恩里克·钱、迈克·克拉克、德里克·伊根、阿南特·纳瓦尔加里亚、坎查娜·帕特洛拉、朱莉娅·维辛格  
- **策划与编辑**：阿南特·纳瓦尔加里亚、坎查娜·帕特洛拉  
- **设计师**：迈克尔·兰宁  

2025年11月  
第2页  


## 目录
1. 从预测型人工智能到自主智能体 ··· 第6页  
2. 智能体（AI Agents）入门 ··· 第8页  
3. 智能体问题解决流程 ··· 第10页  
4. 智能体系统分类 ··· 第14页  
   - 0级：核心推理系统 ··· 第15页  
   - 1级：联网问题解决型智能体 ··· 第15页  
   - 2级：策略性问题解决型智能体 ··· 第16页  
   - 3级：协作式多智能体系统 ··· 第17页  
   - 4级：自进化系统 ··· 第18页  
5. 智能体核心架构：模型、工具与编排层 ··· 第19页  
   - 模型：智能体的“大脑” ··· 第19页  
   - 工具：智能体的“双手” ··· 第20页  
     - 执行操作：改变现实世界 ··· 第21页  
     - 获取信息：扎根现实场景 ··· 第21页  
     - 函数调用：连接工具与智能体 ··· 第22页  
   - 编排层 ··· 第22页  
     - 注入领域知识与设定角色 ··· 第23页  
     - 核心设计选择 ··· 第23页  
     - 补充上下文信息 ··· 第24页  
6. 多智能体系统与设计模式 ··· 第24页  
7. 智能体部署与服务 ··· 第26页  
8. 智能体运维（Agent Ops）：应对不确定性的结构化方案 ··· 第27页  
   - 聚焦关键指标：像A/B测试一样衡量成效 ··· 第29页  
   - 重质量而非“非对即错”：用语言模型（LM）做评估 ··· 第29页  
   - 数据驱动开发：部署的“通行证” ··· 第30页  
   - 用OpenTelemetry追踪调试：搞懂“为什么出错” ··· 第30页  
   - 重视人工反馈：为自动化指引方向 ··· 第31页  
9. 智能体互操作性 ··· 第31页  
   - 智能体与人类 ··· 第32页  
   - 智能体与智能体 ··· 第33页  
10. 单个智能体的安全防护：信任的权衡 ··· 第34页  
    - 限制访问的策略 ··· 第34页  
    - 智能体身份：新型主体类别 ··· 第35页  
    - ADK智能体的安全防护 ··· 第37页  
11. 从单个智能体扩展到企业级智能体集群 ··· 第39页  
    - 安全与隐私：加固智能体领域 ··· 第40页  
    - 智能体治理：用“控制平面”替代“无序扩张” ··· 第40页  
12. 智能体如何进化与学习 ··· 第42页  
    - 智能体的学习与自进化机制 ··· 第43页  
    - 模拟与智能体训练环境（Agent Gym）：下一个前沿领域 ··· 第46页  
13. 高级智能体案例 ··· 第47页  
    - 谷歌协同科学家（Google Co-Scientist） ··· 第47页  
    - AlphaEvolve智能体 ··· 第49页  
14. 结论 ··· 第51页  
15. 附注 ··· 第52页  


## 智能体与智能体架构入门
智能体是语言模型（Language Models）自然进化的产物，能在软件中发挥实际作用。  

### 从预测型人工智能到自主智能体
人工智能正在迎来变革。多年来，大家的注意力都集中在那些擅长处理“被动、零散任务”的模型上——比如回答一个问题、翻译一段文本，或者根据提示生成一张图片。这种模式虽然强大，但每一步都得靠人类不断指导。如今，我们正见证一场范式转移：人工智能正从“只做预测或生成内容”，转向“能自主解决问题、执行任务”的全新软件类别。  

这个新领域的核心就是智能体（AI Agents）。智能体不只是“静态工作流里的一个AI模型”，它是一个完整的应用程序——能制定计划、采取行动来实现目标。它既具备语言模型（LM）的推理能力，又拥有实际行动能力，能处理那些单一模型根本搞不定的复杂多步骤任务。而它最关键的能力在于：能自主工作，不用人类步步指引，自己就能想明白“要达成目标接下来该做什么”。  

本文是一个五篇系列文档的第一篇，专门为开发者、架构师和产品负责人提供正式指导——帮助他们从“概念验证”过渡到“稳健、可用于生产环境的智能体系统”。搭建一个简单的原型并不难，但要确保系统的安全性、质量和可靠性，却是个大挑战。本文会提供全面的基础内容，包括：  
- **核心构成**：将智能体拆解为三个关键组件——负责推理的“模型”、可执行操作的“工具”、以及统筹管理的“编排层”。  
- **能力分类**：给智能体分等级——从简单的“联网问题解决型”，到复杂的“协作式多智能体系统”。  
- **架构设计**：深入讲解每个组件的实际设计考量，从模型选择到工具落地。  
- **生产级搭建**：确立“智能体运维（Agent Ops）”规范——涵盖智能体系统的评估、调试、安全防护，以及从单个实例扩展到企业级集群的治理方案。  

本文基于之前的《智能体白皮书》¹ 和《智能体配套指南》² 撰写，为大家提供核心概念与策略框架，助力成功搭建、部署和管理新一代智能应用——这类应用能通过推理、行动和观察来完成目标³。  

用语言很难准确描述人类与AI的互动方式。我们总喜欢把AI“拟人化”，用“思考”“推理”“知道”这些人类常用的词来形容它。但目前还没有专门的词汇，来区分“基于语义理解的‘知道’”和“为了最大化奖励函数、大概率判断的‘知道’”。这两种“知道”本质不同，但99.X%的情况下，结果是一样的。  


## 智能体（AI Agents）入门
简单来说，AI智能体可以定义为“模型、工具、编排层和运行时服务的组合”——它会通过“循环调用语言模型（LM）”来实现目标。这四个要素构成了任何自主系统的核心架构：  

### 1. 模型（“大脑”）
指核心语言模型（LM）或基础模型，是智能体的中央推理引擎，负责处理信息、评估选项、做出决策。模型的类型（通用型、微调型、多模态型）决定了智能体的认知能力。可以说，智能体系统是语言模型“输入上下文窗口”的“终极管理者”——会精心筛选放进窗口的信息。  

### 2. 工具（“双手”）
这些机制能把智能体的推理能力与现实世界连接起来，让智能体除了生成文本，还能做更多事。工具包括API扩展、代码函数、数据存储（比如数据库或向量数据库）——可用于获取实时、真实的信息。在智能体系统中，语言模型会自主规划“该用哪些工具”，执行工具调用，再把工具返回的结果，填入下一次调用模型时的“输入上下文窗口”。  

### 3. 编排层（“神经系统”）
负责统筹智能体“操作循环”的核心流程，管理规划、记忆（状态）和推理策略的执行。这一层会用“提示框架”和“推理技巧”（比如“思维链（Chain-of-Thought）”⁴ 或“反应式推理（ReAct）”⁵），把复杂目标拆成步骤，判断“什么时候该思考”“什么时候该用工具”。同时，编排层还负责给智能体“记忆能力”——让它能“记住”信息。  

### 4. 部署（“身体与四肢”）
在笔记本电脑上搭建智能体，适合做原型验证；但要让它成为可靠、可访问的服务，还得靠“生产级部署”。这一步需要把智能体部署到安全、可扩展的服务器上，并集成必要的生产级服务（监控、日志、管理）。部署完成后，用户可以通过图形界面访问智能体，其他智能体也能通过“智能体到智能体（A2A）API”以编程方式调用它。  

说到底，搭建生成式AI智能体，是一种解决任务的“新开发思路”。传统开发者像“泥瓦匠”——要精确定义每一步逻辑；而智能体开发者更像“导演”：不用为每个动作写明确代码，而是“搭场景”（设定指导指令和提示）、“选演员”（挑选工具和API）、“给背景”（提供数据）。核心任务变成了“引导这个自主的‘演员’，完成预期的‘表演’”。  

你很快会发现：语言模型最大的优势——超强的灵活性——也可能是最让你头疼的问题。正因为大语言模型“啥都能做”，要让它“稳定、完美地做好某一件特定的事”，反而很难。我们以前叫“提示工程”，现在叫“上下文工程”——其实就是通过优化上下文，引导语言模型生成想要的输出。每次调用语言模型时，我们会输入指令、事实、可用工具列表、示例、对话历史、用户画像等信息，把“上下文窗口”填进“刚好能得到所需输出”的内容。而智能体，就是“通过管理语言模型的输入，来完成工作”的软件。  

出问题时，调试就变得至关重要。“智能体运维（Agent Ops）”本质上是重新定义了“衡量、分析、系统优化”这个熟悉的循环。通过追踪记录（traces）和日志，你能监控智能体的“思考过程”，找出它偏离预期执行路径的地方。随着模型升级、框架优化，开发者的角色会变成“提供关键组件”：领域专业知识、明确的角色设定，以及与“完成实际任务所需工具”的无缝集成。要记住：全面的评估和检验，往往比最初的提示更能决定智能体的表现。  

如果一个智能体配置得当——有清晰的指令、可靠的工具、集成了“记忆功能”的上下文、好用的用户界面，还能规划任务、解决问题，并且具备通用常识——那它就不再是“简单的工作流自动化工具”了。它会变成一个“协作伙伴”：高效、适应性强、能力出色，像新加入团队的成员一样。  

本质上，智能体是一个“专注于管理上下文窗口”的系统。它会不断循环：整理上下文→提示模型→观察结果→为下一步重新整理上下文。这里的“上下文”可能包括系统指令、用户输入、对话历史、长期记忆、来自权威来源的“事实依据”、可用工具列表，以及已调用工具的结果。通过这种对“模型注意力”的精细管理，智能体的推理能力能应对新场景、完成目标。  


## 智能体问题解决流程
我们已经把AI智能体定义为“完整的、以目标为导向的应用”——整合了推理模型、可执行工具和统筹编排层。简单说，就是“带工具的语言模型循环调用系统”，用来完成目标。  

但这个系统实际是怎么工作的？从收到请求到给出结果，智能体到底做了些什么？  

智能体的核心运作逻辑，是通过“持续循环”来实现目标。这个循环可能变得很复杂，但可以拆解成五个基本步骤（《智能体系统设计》⁶ 一书中有详细讲解）：  

### 步骤1：明确任务（Get the Mission）
流程从一个具体的“高阶目标”开始。这个目标可能来自用户（比如“帮我团队安排即将到来的会议差旅”），也可能来自自动触发（比如“收到一个高优先级客户工单”）。  

### 步骤2：扫描场景（Scan the Scene）
智能体会“感知环境”，收集上下文信息。这一步由编排层负责，调用可用资源：比如“用户的请求里说了什么？”“我的短期记忆里有什么？我之前试过做这个任务吗？用户上周给过指引吗？”“通过工具（日历、数据库、API）能获取什么信息？”  

### 步骤3：思考规划（Think It Through）
这是智能体的核心“思考”环节，由推理模型驱动。智能体会结合“任务（步骤1）”和“场景（步骤2）”，制定计划。这不是“单一想法”，而是一连串推理，比如：“要订差旅，首先得知道团队有谁——我要用get_team_roster工具查一下。然后得通过calendar_api查他们的 availability（空闲时间）。”  

### 步骤4：采取行动（Take Action）
编排层会执行计划中的第一个具体步骤：选择并调用合适的工具——可能是调用API、运行代码函数，或是查询数据库。这一步是智能体“作用于现实世界”的关键，不再局限于内部推理。  

### 步骤5：观察迭代（Observe and Iterate）
智能体会观察行动的结果。比如调用get_team_roster工具后，得到5个人的名单——这个新信息会被加入智能体的“上下文”或“记忆”。之后循环会回到步骤3：“现在有了名单，下一步要查这5个人的日历——我要用calendar_api。”  

这个“思考→行动→观察”的循环会一直持续——由编排层统筹、模型推理、工具执行——直到智能体完成内部计划，达成最初的任务目标。  

（图1：智能体问题解决流程——步骤依次为：明确任务→扫描场景→思考规划→采取行动→学习优化）  

我们用“客户支持智能体”的真实案例，看看这五个步骤怎么运作：  
假设用户问：“我的订单#12345在哪儿？”  

智能体不会马上行动，而是先进入“思考规划”阶段，制定完整策略。它会这样推理：“用户想知道物流状态，要给出完整答案，得分三步：  
1. 确认订单：先在内部数据库里找到这个订单，确认存在并获取详情。  
2. 追踪物流：从订单详情里提取快递公司的追踪号，再调用外部快递公司的API查实时状态。  
3. 反馈结果：把收集到的信息整理成清晰易懂的回复，发给用户。”  

有了这个多步骤计划，智能体就开始执行：  
- 第一次“行动”：执行计划第一步，调用find_order("12345")工具。观察结果——拿到完整订单记录，包括追踪号“ZYX987”。  
- 编排层发现第一步完成，立刻进入第二步：调用get_shipping_status("ZYX987")工具。观察结果——“正在派送中”。  
- 最后，完成信息收集后，智能体进入“反馈结果”步骤：判断已有足够信息，规划最终回复内容，生成并发送：“您的订单#12345正在派送中！”  


## 智能体系统分类
理解“五步操作循环”是第一步，第二步是要意识到：这个循环的复杂度可以不断升级，进而形成不同类别的智能体。对架构师或产品负责人来说，初期的关键决策之一，就是“明确要搭建哪种级别的智能体”。  

我们可以把智能体系统分成几个大的等级，每个等级都基于前一个等级的能力构建而成：  

（图2：智能体系统五级分类——从0级“核心推理系统”到4级“自进化系统”）  


### 0级：核心推理系统（The Core Reasoning System）
要搭建智能体，得先有最基础的“大脑”——也就是推理引擎本身。在这个阶段，语言模型（LM）是“孤立运行”的：只靠预训练时学到的海量知识来响应，没有工具、没有记忆，也无法与实时环境互动。  

它的优势在于“训练数据丰富”——能深入解释已有的概念，也能规划“解决问题的思路”。但缺点也很明显：完全没有“实时感知能力”，对训练数据之外的事件或事实，相当于“看不见”。  

比如，它能解释职业棒球的规则，能讲清纽约洋基队的完整历史，但如果你问“昨晚洋基队比赛的最终比分是多少”，它就答不上来了。因为那场比赛是“训练数据收集之后”发生的真实事件，相关信息根本不在它的知识库的里。  


### 1级：联网问题解决型智能体（The Connected Problem-Solver）
到了这个等级，推理引擎会“连接并使用外部工具”——也就是我们之前说的“双手”组件，从而变成一个能实际用的智能体。它的问题解决能力，不再局限于静态的预训练知识。  

通过“五步循环”，智能体现在能回答刚才那个“洋基队比分”的问题了。比如收到任务“查昨晚洋基队比赛的最终比分”，它的“思考”环节会意识到“需要实时数据”，然后“行动”环节调用工具（比如谷歌搜索API，带上正确的日期和关键词），“观察”到搜索结果（比如“洋基队5-3赢了”），最后把这个事实整理成最终答案。  

这种“与现实世界互动的能力”——不管是用搜索工具查比分、用金融API查实时股价，还是通过“检索增强生成（RAG）”查询数据库——是1级智能体的核心能力。  


### 2级：策略性问题解决型智能体（The Strategic Problem-Solver）
2级智能体的能力有了显著提升：从“执行简单任务”升级到“能制定策略，处理复杂的多步骤目标”。这个等级的关键技能是“上下文工程”——智能体能主动筛选、整合、管理“每一步计划所需的关键信息”。  

智能体的准确性，依赖于“聚焦、高质量的上下文”。而“上下文工程”会精心管理模型的“注意力范围”——避免信息过载，确保高效运作。  

举个例子，任务是“在我办公室（加州山景城Amphitheatre Parkway 1600号）和客户办公室（旧金山Market街1号）之间，找一家好的咖啡店”。  

2级智能体会这样制定计划：  
1. 思考：“得先找中点位置。”  
   - 行动：调用地图工具，输入两个地址。  
   - 观察：“中点是加州米尔布雷（Millbrae）。”  
2. 思考：“现在要在米尔布雷找咖啡店。用户要‘好的’，那我就搜评分4星及以上的。”  
   - 行动：调用google_places工具，查询关键词“Millbrae, CA的咖啡店”，筛选条件“最低评分4.0”。（这就是上下文工程——它会自动用前一步的结果，生成新的、精准的搜索指令。）  
   - 观察：“搜索结果有‘Millbrae Coffee’和‘The Daily Grind’。”  
3. 思考：“把结果整理一下，发给用户。”  

这种策略性规划还能实现“主动协助”——比如智能体读了一封长长的航班确认邮件后，会自动提取关键上下文（航班号、日期），然后行动起来，把信息添加到你的日历里。  


### 3级：协作式多智能体系统（The Collaborative Multi-Agent System）
到了这个最高等级，范式完全变了。我们不再追求“单个全能‘超级智能体’”，而是转向“专业智能体团队协作”——这和人类组织的运作模式很像。系统的整体优势，就来自“分工合作”。  

在这个系统里，智能体会把“其他智能体”当成工具来用。比如有个“项目经理智能体”，收到任务“发布新款‘Solaris’耳机”。它不会自己包揽所有工作，而是像现实中的项目经理一样，给“团队里的专业智能体”分配子任务：  
1. 给“市场调研智能体”派任务：“分析降噪耳机的竞品定价，明天之前提交总结文档。”  
2. 给“营销智能体”派任务：“根据‘Solaris’产品规格表，写3版新闻稿。”  
3. 给“网页开发智能体”派任务：“根据附带的设计稿，生成新产品页面的HTML代码。”  

这种协作模式目前还受限于“当前语言模型的推理能力”，但它代表了“自动化完成复杂业务全流程”的前沿方向。  


### 4级：自进化系统（The Self-Evolving System）
4级智能体实现了从“分配任务”到“自主创建、自适应”的巨大飞跃。在这个等级，智能体系统能“发现自身能力缺口”，并动态创建新工具——甚至新智能体——来填补缺口。它不再局限于“使用固定资源”，而是能主动“扩展资源库”。  

接着用“项目经理智能体负责Solaris耳机发布”的例子：它可能意识到“需要监控社交媒体舆情”，但团队里没有对应的工具或智能体。这时它会：  
1. 思考（元推理）：“我得追踪‘Solaris’的社交媒体讨论，但没这个能力。”  
2. 行动（自主创建）：不会就此放弃，而是调用一个高阶工具“AgentCreator”，下达新任务：“搭建一个新智能体——能监控包含‘Solaris耳机’关键词的社交媒体内容，做情感分析，每天提交总结报告。”  
3. 观察：新的“情感分析智能体”会被实时创建、测试，然后加入团队，随时为最初的发布任务出力。  

这种“能动态扩展自身能力”的自主性，让“智能体团队”变成了一个真正“会学习、能进化”的组织。  


## 智能体核心架构：模型、工具与编排层
我们已经知道智能体“能做什么”以及“能力如何分级”，但实际该怎么搭建它呢？从概念到代码的关键，在于“三个核心组件”的具体架构设计。  


### 模型：智能体的“大脑”
语言模型（LM）是智能体的推理核心，选择模型是至关重要的架构决策——它会决定智能体的认知能力、运行成本和速度。但要是把“选模型”当成“挑基准测试分数最高的”，就很容易出错。在生产环境中，智能体的成功与否，很少由“通用学术基准”决定。  

现实场景的成功，需要模型“在智能体核心能力上表现突出”：一是能应对复杂多步骤问题的“超强推理能力”，二是能与现实世界互动的“可靠工具使用能力”⁷。  

要做好这一点，首先得明确“业务问题是什么”，然后针对“与业务结果直接相关的指标”测试模型。比如你的智能体要写代码，就用公司的私有代码库来测试；要处理保险理赔，就评估它“从特定格式文档里提取信息”的能力。之后，还要结合“成本和延迟”这些实际因素综合判断。对你的具体任务来说，“最好的模型”是在“质量、速度、价格”三者间找到最优平衡点的那个⁸。  

你也可以选择“多个模型搭配使用”——就像“专业团队协作”。毕竟“杀鸡不用牛刀”：稳健的智能体架构，可能会用“Gemini 2.5 Pro”这类前沿模型来承担“初期规划”和“复杂推理”的重活；但对于“用户意图分类”“文本总结”这类简单、高频的任务，会智能路由到“Gemini 2.5 Flash”这类更快、更便宜的模型。模型路由可以是自动的，也可以是硬编码的——这是优化性能和成本的关键策略⁹。  

处理多种数据类型时，也可以用类似思路。虽然“Gemini实时模式（Gemini live mode）”¹⁰ 这类原生多模态模型，能简化“处理图像和音频”的流程，但也可以选择“用专业工具”——比如Cloud Vision API¹¹ 或Speech-to-Text API¹²。这种模式下，会先把“现实世界的信息”（图像、音频）转换成文本，再传给“纯语言模型”做推理。虽然灵活性更高，能选“各领域最好的组件”，但也会增加系统复杂度。  

最后要注意：AI领域的变化太快了，你现在选的模型，半年后可能就会被淘汰。“一劳永逸”的想法不现实。要应对这种情况，就得投入搭建“灵活的运维框架”——也就是“智能体运维（Agent Ops）”规范¹³。有了稳健的CI/CD流水线，能“持续用关键业务指标评估新模型”，就能降低升级风险、加快升级速度，确保智能体始终用“最好的大脑”，而不用彻底重构架构。  


### 工具：智能体的“双手”
如果说模型是智能体的“大脑”，那工具就是它“连接现实、付诸行动的双手”。工具能让智能体突破“静态预训练数据”的限制，获取实时信息，在现实世界中做事。一个稳健的工具接口，是“三步循环”：明确工具功能、调用工具、观察结果。  

下面介绍几种智能体开发者常用的核心工具类型（更详细的内容，可参考本系列中“聚焦智能体工具”的白皮书）：  

#### 1. 获取信息：扎根现实场景（Retrieving Information: Grounding in Reality）
最基础的工具能力，是“获取最新信息”。“检索增强生成（RAG）”就像给智能体发了一张“图书馆借书卡”——能查询外部知识库（通常存在向量数据库或知识图谱里），内容可以是公司内部文档，也可以是通过谷歌搜索获取的网络知识。  

对于结构化数据，“自然语言转SQL（NL2SQL）”工具能让智能体查询数据库，回答分析类问题（比如“上季度我们最畅销的产品是什么？”）。不管是查文档还是查数据库，智能体“先查证再回答”的模式，能让它“基于事实说话”，大幅减少“胡编乱造（幻觉）”的情况。  

#### 2. 执行操作：改变现实世界（Executing Actions: Changing the World）
智能体的真正威力，体现在“从‘读信息’到‘做事情’的跨越”。把现有API和代码函数“包装成工具”后，智能体就能发邮件、订会议、更新ServiceNow里的客户记录。对于更动态的任务，智能体还能“实时写代码、执行代码”——在安全的沙箱环境里，它可以生成SQL查询语句或Python脚本，解决复杂问题或做计算，从“知识助手”变成“自主行动者”¹⁴。  

工具还包括“人机交互工具”。智能体可以用“人机协同（HITL）工具”暂停工作流，向人类确认（比如调用ask_for_confirmation()函数），或通过用户界面请求特定信息（比如调用ask_for_date_input()函数）——确保关键决策有人类参与。这种人机协同功能，甚至可以通过短信和数据库任务来实现。  

#### 3. 函数调用：连接工具与智能体（Function Calling: Connecting Tools to your Agent）
要让智能体“可靠地调用函数、使用工具”，需要清晰的指令、安全的连接和统筹管理¹⁵。像OpenAPI规范这种成熟标准，就能提供帮助——它会给智能体一个“结构化协议”，说明工具的用途、必填参数、预期返回结果。有了这个协议，模型就能每次都生成正确的函数调用，还能理解API返回的内容。  

如果想更简单地“发现工具、连接工具”，像“模型上下文协议（MCP）”这类开放标准就很受欢迎——用起来更方便¹⁶。另外，有些模型自带工具，比如Gemini能原生调用谷歌搜索——函数调用直接集成在“调用语言模型”的步骤里¹⁷。  


### 编排层（The Orchestration Layer）
如果说模型是“大脑”、工具是“双手”，那编排层就是“连接两者的中枢神经系统”。它是“思考→行动→观察”循环的驱动引擎，是控制智能体行为的“状态机”，也是开发者精心设计的逻辑“落地载体”。编排层不只是“管道”，更是“整个智能体系统的指挥家”——决定什么时候让模型推理、该调用哪个工具、以及如何用工具结果指导下一步行动。  

#### 1. 核心设计选择（Core Design Choices）
第一个架构决策，是“确定智能体的自主程度”——这是个“ spectrum（连续范围）”：  
- 一端是“确定性、可预测的工作流”：把语言模型当成“完成特定任务的工具”，在现有流程里加一点AI增强。  
- 另一端是“语言模型主导”：让它动态调整、规划和执行任务，自主实现目标。  

另一个并行决策是“实现方式”：  
- “无代码构建工具”：速度快、易上手，能让业务人员快速自动化结构化任务、搭建简单智能体。  
- “代码优先框架”：比如谷歌的“智能体开发工具包（ADK）”¹⁸——适合复杂、关键业务系统，能给工程师提供深度控制、定制化和集成能力。  

不管选哪种方式，“生产级框架”都必不可少，而且要满足三个要求：  
- **开放性**：能接入任何模型或工具，避免“绑定单一供应商”。  
- **精准控制**：支持“混合模式”——用硬编码的业务规则，约束语言模型的“非确定性推理”。  
- **可观测性**：智能体行为异常时，你没法在模型的“思考过程”里设断点调试。所以框架必须生成详细的“追踪记录（traces）”和日志，暴露完整的推理轨迹——包括“发给模型的具体提示”“模型的内部推理（如果能获取）”“选了哪个工具”“生成的工具参数”“观察到的结果”。  

#### 2. 注入领域知识与设定角色（Instruct with Domain Knowledge and Persona）
在这个框架里，开发者最有力的“杠杆”，是“给智能体注入领域知识、设定明确角色”——通过“系统提示（system prompt）”或“核心指令集”实现。这不是简单的命令，而是智能体的“宪法”。  

比如你可以这样定义：“你是Acme公司的客户支持智能体，……”——同时给出约束条件、期望的输出格式、互动规则、特定语气，以及“什么时候该用工具、为什么要用工具”的明确指引。在指令里加几个示例场景，效果通常很好。  

#### 3. 补充上下文信息（Augment with Context）
智能体的“记忆”，会在运行时被“编排进语言模型的上下文窗口”（更详细的内容，可参考本系列中“聚焦智能体记忆”的白皮书）。  

- **短期记忆**：相当于智能体的“实时草稿本”，记录当前对话的历史，追踪“行动→观察”的循环序列——给模型提供“下一步决策所需的即时上下文”。实现方式可以是“状态（state）”“工件（artifacts）”“会话（sessions）”“线程（threads）”这类抽象概念。  
- **长期记忆**：实现“跨会话持久化”。从架构上看，几乎都是用“专业工具”实现——比如连接向量数据库或搜索引擎的RAG系统。编排层会让智能体“主动预加载”或“查询自身历史”，比如记住用户的偏好，或几周前类似任务的结果，从而提供“个性化、连贯的体验”¹⁹。  


## 多智能体系统与设计模式
任务变复杂后，搭建“单个全能‘超级智能体’”会越来越低效。更有效的方案是“专业团队协作”——和人类组织的模式一样。这就是多智能体系统的核心：把复杂流程拆成“独立子任务”，每个子任务交给“专门的AI智能体”处理。这种分工能让每个智能体更简单、更专注，也更容易搭建、测试和维护——非常适合“动态或长期运行的业务流程”。  

架构师可以借鉴成熟的“智能体设计模式”（不过智能体能力还在快速进化，模式也在更新²⁰）。比如处理“动态或非线性任务”时，“协调者模式（Coordinator Pattern）”就很关键：会有一个“管理者智能体”分析复杂请求，拆分主任务，把每个子任务智能分配给对应的专业智能体（比如研究员、写手、程序员），最后汇总所有专业智能体的结果，形成完整答案。  

（图3：“迭代优化”模式——来源：https://cloud.google.com/architecture/choose-design-pattern-agentic-ai-system，流程为：用户输入→提示生成器→提示增强器→循环智能体/子智能体→质量评估智能体→若评分达标则输出，若不达标则返回优化）  

对于“线性工作流”，“顺序模式（Sequential Pattern）”更合适——像数字流水线，前一个智能体的输出直接作为下一个的输入。还有些关键模式聚焦“质量和安全”：比如“迭代优化模式（Iterative Refinement Pattern）”会建立反馈循环——让“生成智能体”创作内容，“评估智能体”根据质量标准打分；对于“高风险任务”，“人机协同（HITL）模式”很关键——会在工作流中刻意暂停，等人类确认后再让智能体执行重要操作。  


## 智能体部署与服务
在本地搭建好智能体后，你会想把它部署到服务器上——让它一直运行，方便其他人或其他智能体使用。继续用之前的“身体比喻”：部署和服务相当于智能体的“身体和四肢”。一个有用的智能体需要多种服务支持，比如会话历史存储、记忆持久化等。作为智能体开发者，你还要决定“该记录哪些日志”“采取哪些安全措施”——确保数据隐私、符合数据驻留和监管要求。这些都是“智能体生产级部署”要考虑的内容。  

幸运的是，智能体开发者可以借助“几十年的应用托管基础设施”。毕竟智能体也是一种新软件，很多传统原则依然适用。开发者可以用“专门为智能体设计的部署方案”，比如Vertex AI Agent Engine——这个平台集成了运行时和其他所需功能，一站式搞定²¹。如果软件开发者想“更直接地控制应用栈”，或“在现有DevOps基础设施里部署智能体”，也可以把智能体（以及大部分智能体服务）装进Docker容器，部署到Cloud Run或GKE这类行业标准运行时环境²²。  

（图4：Vertex AI智能体搭建工具——包含客户端、智能体引擎、智能体框架、工具、模型等模块，来源略）  

如果你不是软件开发者，也不是DevOps专家，第一次部署智能体可能会觉得难。不过很多智能体框架都简化了这个过程——比如提供“部署命令”或“专用部署平台”，适合初期探索和上手。但要升级到“安全、可用于生产的环境”，通常需要投入更多时间，落实最佳实践，包括智能体的CI/CD和自动化测试²³。  


## 智能体运维（Agent Ops）：应对不确定性的结构化方案
搭建第一个智能体时，你会反复手动测试它的行为：加个功能，能跑通吗？修个bug，会不会引入新问题？测试对软件开发来说很正常，但在生成式AI领域，测试方式不一样。  

从“传统确定性软件”转向“随机性智能体系统”，需要新的运维理念。传统软件的单元测试，可以简单断言“输出==预期结果”；但智能体的响应“天生带随机性”，这种测试方法行不通。而且语言本身很复杂，判断“质量”（比如智能体的回答是否全面、无多余内容、语气恰当）通常需要用语言模型（LM）来评估。  

（图5：DevOps、MLOps和GenAIOps的运维领域关系——来源：https://medium.com/@sokratis.kartakis/genai-in-production-mlops-or-genaiops-25691c9becd0，GenAIOps包含LLMOps、FMOps、AgentOps等子领域）  

智能体运维（Agent Ops）就是“应对这种新现实的严谨结构化方案”。它是DevOps和MLOps的自然延伸，专为AI智能体的“搭建、部署、治理”挑战设计，能把“不确定性”从“隐患”变成“可管理、可衡量、可信赖的特性”²⁴（更详细的内容，可参考本系列中“聚焦智能体质量”的白皮书）。  


### 1. 聚焦关键指标：像A/B测试一样衡量成效（Measure What Matters: Instrumenting Success Like an A/B Experiment）
要改进智能体，首先得明确“在你的业务场景里，‘更好’是什么意思”。把可观测性策略当成A/B测试来设计，问自己：“哪些关键绩效指标（KPIs）能证明智能体在创造价值？”这些指标不能只看“技术正确性”，还要衡量“现实影响”，比如：目标完成率、用户满意度评分、任务延迟、每次交互的运营成本——最重要的是，对“收入、转化率、客户留存”等业务目标的影响。这种“从业务出发”的视角，会指导后续的所有测试，让你走上“数据驱动开发”的道路，还能计算投资回报率（ROI）。  


### 2. 重质量而非“非对即错”：用语言模型（LM）做评估（Quality Instead of Pass/Fail: Using a LM Judge）
业务指标没法告诉你“智能体的行为是否合规”。既然“非对即错”的判断不现实，我们就转向“用‘语言模型当评估者（LM as Judge）’来衡量质量”。具体来说，就是用一个强大的模型，根据“预设标准”评估智能体的输出：答案对吗？内容有事实依据吗？有没有按指令来？  

这种自动化评估会“基于一组标准提示语（黄金数据集）”运行，能提供“一致的质量衡量标准”。不过创建“评估数据集”（包含“标准问题”和“正确答案”）是个繁琐的活儿。建议从“智能体现有生产或开发环境的交互记录”中抽样场景，覆盖“用户可能用到的所有场景”，再加上几个“意外场景”。虽然投入评估工作很快能看到回报，但评估结果一定要“让领域专家审核后，才能确认有效”。现在，“整理和维护评估数据集”正逐渐成为产品经理的核心职责——当然，会有领域专家提供支持。  


### 3. 数据驱动开发：部署的“通行证”（Metrics-Driven Development: Your Go/No-Go for Deployment）
当你完成了几十个“自动化评估场景”，确立了“可信的质量分数”后，就能自信地测试“开发环境智能体的变更”了。流程很简单：让新版本智能体跑遍所有评估场景，把它的分数和“现有生产版本”直接对比。这个稳健的系统能消除“靠感觉判断”的情况，让你每次部署都有底气。  

不过，自动化评估虽然关键，但也不能忽略“延迟、成本、任务成功率”等其他重要因素。为了最大化安全性，可以用“A/B部署”——慢慢推出新版本，同时对比“现实生产环境的指标”和“模拟测试的分数”。  


### 4. 用OpenTelemetry追踪调试：搞懂“为什么出错”（Debug with OpenTelemetry Traces: Answering "Why?"）
当指标下降或用户反馈bug时，你需要知道“为什么会这样”。OpenTelemetry追踪记录（trace）是“高保真、一步一步的智能体执行轨迹记录”，能帮你调试智能体的每一步²⁵。通过追踪记录，你能看到：发给模型的具体提示、模型的内部推理（如果能获取）、选了哪个工具、生成的工具参数细节、以及返回的原始数据。  

第一次看追踪记录可能会觉得复杂，但它能提供“定位根本问题”所需的细节。虽然追踪记录里的关键信息可以转成指标，但“查看追踪记录”主要用于调试，不是为了“概览性能”。追踪数据可以无缝集成到“谷歌Cloud Trace”这类平台——这些平台能可视化和搜索大量追踪记录，简化“根因分析”流程。  


### 5. 重视人工反馈：为自动化指引方向（Cherish Human Feedback: Guiding Your Automation）
人工反馈不是“麻烦事”，而是“改进智能体最宝贵、数据最丰富的资源”。用户提交bug报告，或点“踩”按钮时，其实是在给你“礼物”：一个“自动化评估场景没覆盖到的、真实世界的边缘案例”。收集和汇总这些数据至关重要——当你发现“类似反馈或指标异常达到统计显著水平”时，一定要把这些情况和“分析平台”关联起来，提炼洞察，触发“运维问题警报”。  

一套有效的智能体运维流程，会“形成闭环”：捕捉反馈→复现问题→把这个场景变成“评估数据集里的永久测试用例”。这样不仅能修复当前bug，还能“给系统打疫苗”——避免同类错误再发生。  


## 智能体互操作性
搭建好高质量智能体后，你会想让它“和用户、其他智能体互联互通”。继续用“身体比喻”：这相当于智能体的“脸”——负责对外交互。要注意，“连接智能体”和“让智能体连接数据、API”是两回事；智能体不是工具²⁶。假设你已经给智能体配好了工具，现在来看看“如何把智能体融入更广泛的生态”。  


### 1. 智能体与人类（Agents and Humans）
智能体与人类最常见的交互方式，是通过“用户界面（UI）”。最简单的就是聊天机器人：用户输入请求，智能体作为后端服务处理后，返回一段文本。更高级的智能体，能提供JSON这类结构化数据，支持“丰富、动态的前端体验”。  

“人机协同（HITL）”的交互模式有很多种，比如：细化用户意图、扩展目标、确认操作、请求补充信息。  

还有一种“工具类别”叫“计算机控制”——语言模型会“操控用户界面”，通常需要人类参与和监督。比如“支持计算机控制的智能体”会判断：下一步最好“导航到新页面”“高亮某个按钮”，或“用相关信息预填表单”²⁷。  

除了“智能体代用户操作界面”，语言模型还能“根据实时需求调整界面”。实现方式有几种：用“控制UI的工具（MCP UI）”²⁸，或“专用UI消息系统（同步客户端状态和智能体，如AG UI）”²⁹，甚至“生成定制界面（A2UI）”³⁰。  

当然，人类与智能体的交互不局限于“屏幕和键盘”。高级智能体正突破“文本限制”，进入“实时多模态交互”——比如“实时模式（live mode）”，能创造更自然、更像人类对话的连接。像“Gemini Live API”³¹ 这类技术支持“双向流式传输”，用户可以“和智能体说话”，还能像日常对话一样“打断它”。  

这种能力从根本上改变了“人类与智能体协作”的性质。智能体只要能访问设备的摄像头和麦克风，就能“看到用户所见、听到用户所说”，还能“生成语音回复”——延迟低到和人类对话差不多。这打开了“纯文本交互做不到”的大量场景：比如技术人员修设备时，能“免手操作”获取智能体指导；购物者能实时获得穿搭建议。智能体从此变成了“更直观、更易获取的合作伙伴”。  


### 2. 智能体与智能体（Agents and Agents）
智能体既要和人类交互，也要和其他智能体互动。企业大规模使用AI时，不同团队会搭建不同的专业智能体。如果没有通用标准，连接这些智能体就需要“搭建一堆脆弱的定制API集成”——根本没法维护。核心挑战有两个：一是“发现（我的智能体怎么找到其他智能体，知道它们能做什么）”，二是“通信（怎么确保它们‘说同一种语言’）”。  

“智能体到智能体（A2A）协议”就是为解决这个问题设计的开放标准，相当于“智能体生态的通用握手协议”。通过A2A，任何智能体都能发布“数字名片（Agent Card）”——这是一个简单的JSON文件，会展示智能体的能力、网络端点、交互所需的安全凭证。这让“发现智能体”变得简单又标准。和“专注于处理事务性请求的MCP”不同，A2A通信通常用于“进一步解决问题”。  

找到对方后，智能体之间会通过“任务导向架构”通信。不再是简单的“请求-响应”，而是把交互变成“异步任务”：客户端智能体给服务器智能体发“任务请求”，服务器智能体可以通过“长连接”，在处理过程中实时发送更新。这种稳健、标准化的通信协议，是“实现3级协作式多智能体系统”的最后一块拼图——能把“一堆孤立的智能体”变成“真正可互通的生态”。  


### 3. 智能体与金钱（Agents and Money）
随着AI智能体帮我们做更多事，其中有些任务会涉及“买卖、谈判或促成交易”。现在的互联网是“为人类点击‘购买’按钮设计的”，责任由人类承担。但如果是“自主智能体点击‘购买’”，就会出现“信任危机”——出问题了，该怪谁？这里面涉及“授权、真实性、问责制”等复杂问题。要实现真正的“智能体经济”，需要新的标准，让智能体“代表用户安全、可靠地完成交易”。  

这个领域还处于早期阶段，但有两个关键协议正在铺路：  
- **智能体支付协议（AP2）**：这是一个开放协议，旨在成为“智能体商业交易的通用语言”。它在A2A等协议的基础上，引入了“加密签名的数字授权书（mandates）”——能作为“用户意图的可验证证据”，为每笔交易建立“不可否认的审计轨迹”。有了它，智能体就能“基于用户授权”，在全球范围内安全地浏览、谈判和交易。  
- **x402协议**：这是一个开放的互联网支付协议，利用标准HTTP 402状态码（“需要支付”）工作。它能实现“无摩擦的机器对机器小额支付”——智能体可以“按次付费”使用API或购买数字内容，不用复杂的账户或订阅流程。  

这两个协议一起，正在为“智能体互联网”搭建“关键的信任层”。  


## 单个智能体的安全防护：信任的权衡
搭建第一个AI智能体时，你马上会面临一个核心矛盾：“实用性”和“安全性”的权衡。要让智能体有用，就得给它“权力”——自主做决策的能力，以及“发邮件、查数据库”等执行操作的工具。但你给的权力越多，风险也越大。主要的安全顾虑有两个：一是“异常操作（意外或有害行为）”，二是“敏感数据泄露”。你需要给智能体“足够长的‘绳子’完成工作”，但又不能长到让它“闯祸”——尤其是涉及“不可逆转的操作”或“公司私密数据”时³²。  

要应对这个问题，不能只靠AI模型的“判断力”——它可能会被“提示注入（prompt injection）”³³ 这类技术操控。正确的做法是“混合防御、深度防护”³⁴：  

### 1. 第一层：传统确定性防护栏
也就是“硬编码规则”——在模型推理之外，设置“安全关卡”。比如用“策略引擎”限制“单次购买金额不超过100美元”，或“智能体调用外部API前必须经用户明确确认”。这一层能给智能体的“权力”设定“可预测、可审计的硬限制”。  

### 2. 第二层：基于推理的防护
用AI来“保护AI”。具体包括：  
- 训练模型“更能抵御攻击”（对抗训练）；  
- 用小型专业“防护模型（guard models）”——像安全分析师一样，在智能体执行计划前“审查方案”，标记“有风险或违反策略的步骤”，提交审核。  

这种“代码的刚性确定性 + AI的上下文感知”的混合模式，能为单个智能体打造“稳健的安全态势”，确保它的“权力”始终和“目标”对齐。  


### 智能体身份：新型主体类别（Agent Identity: A New Class of Principal）
在传统安全模型里，有两种主体：一是“人类用户”（用OAuth或SSO认证），二是“服务”（用IAM或服务账户）。而智能体带来了第三种主体。智能体不只是一段代码，而是“自主行动者”——需要“可验证的专属身份”。就像员工有工牌一样，平台上的每个智能体都应该有“安全、可验证的数字护照（Agent Identity）”。  

这种“智能体身份”，和“调用它的用户身份”“搭建它的开发者身份”是分开的。这是企业“身份与访问管理（IAM）”的根本转变。  

“每个身份都可验证”+“所有身份都有访问控制”，是智能体安全的基石。给智能体分配“加密可验证的身份”（通常用SPIFFE³⁵ 这类标准）后，就能给它“最小权限”——比如“销售智能体（SalesAgent）”有CRM的读写权限，而“HR入职智能体（HRonboardingAgent）”则明确被拒绝访问CRM。这种“精细化控制”至关重要：即使某个智能体被攻破或行为异常，“影响范围”也能被控制住。没有“智能体身份”这个概念，智能体就没法“代表人类、在有限授权下工作”。  

（表1：不同主体类别的认证示例——非完整列表）  
| 主体类型       | 认证/验证方式       | 说明                                   |
|----------------|---------------------|----------------------------------------|
| 用户           | OAuth或SSO认证      | 人类行动者，拥有完全自主权，对自身行为负责 |
| 智能体（新类别）| SPIFFE验证          | 拥有委托授权，代表用户执行操作           |
| 服务账户       | 集成到IAM           | 应用程序和容器，行为完全确定，无需对行为负责 |  


### 限制访问的策略（Policies to Constrain Access）
“策略（Policy）”是“授权（AuthZ）”的一种形式，和“认证（AuthN）”不同。通常，策略会限制主体的能力，比如“市场部用户只能访问这27个API端点，不能执行DELETE命令”。在搭建智能体时，我们需要给“智能体、工具、其他内部智能体、可共享的上下文、外部智能体”都设置权限。可以这么理解：如果你给系统加了“所有API、数据、工具和智能体”，那就要“限制访问范围”——只开放“完成工作必需的那些能力”。这是推荐的做法：“遵循最小权限原则，同时确保与场景相关”³⁶。  


### ADK智能体的安全防护（Securing an ADK Agent）
明确了“身份”和“策略”的核心原则后，“给用智能体开发工具包（ADK）搭建的智能体做安全防护”，就变成了“通过代码和配置落实这些原则”的具体工作³⁷。  

如前所述，第一步是“明确身份定义”：用户账户（比如OAuth）、服务账户（运行代码用）、智能体身份（使用委托授权）。处理完认证后，下一层防护是“制定策略，限制对服务的访问”——通常在“API治理层”实现，同时还要支持MCP和A2A服务的治理。  

再下一层，是“在工具、模型和子智能体中加入防护栏，执行策略”。这样一来，不管语言模型怎么推理，或恶意提示怎么诱导，“工具自身的逻辑”都会拒绝执行“不安全或违反策略的操作”。这种方式能提供“可预测、可审计的安全基线”，把“抽象的安全策略”变成“具体、可靠的代码”³⁸。  

如果想“根据智能体运行时行为动态调整安全策略”，ADK提供了“回调（Callbacks）”和“插件（Plugins）”功能：  
- “before_tool_callback（工具调用前回调）”：能在工具运行前“检查调用参数”，结合智能体当前状态验证，防止“行为偏离目标”。  
- “插件”：适合实现可复用的策略。比如常见的“Gemini当评估者（Gemini as a Judge）”³⁹ 模式——用“Gemini Flash-Lite”这类快速、低成本的模型，或你自己微调的Gemma模型，实时筛查“用户输入”和“智能体输出”，防范提示注入或有害内容。  

如果企业想“用完全托管的企业级方案做这些动态检查”，可以集成“Model Armor”作为可选服务。Model Armor是“专门的安全层”，能筛查提示和响应中的多种威胁，包括提示注入、越狱尝试、敏感数据（PII）泄露、恶意URL⁴⁰。把这些复杂的安全任务“交给专用服务”，开发者就能不用自己搭建和维护防护栏，同时确保“一致、稳健的防护”。  

ADK的这种混合防护方案——结合“强身份认证、工具内确定性逻辑、AI驱动的动态防护栏、可选托管服务（如Model Armor）”——能帮你搭建“既强大又可信”的单个智能体。  


## 从单个智能体扩展到企业级智能体集群
“单个AI智能体在生产环境跑通”是个胜利，但“扩展到数百个智能体的集群”，就是个架构挑战了。搭建一两个智能体时，你主要担心安全问题；但搭建很多智能体时，就得设计“能应对更复杂情况”的系统。就像“API泛滥”一样，当智能体和工具在企业内遍地开花，会形成“复杂的交互网络、数据流和潜在安全漏洞”。要管理这种复杂度，需要“更高层级的治理层”——整合所有身份和策略，通过“中央控制平面”统一管理。  


### 安全与隐私：加固智能体领域（Security and Privacy: Hardening the Agentic Frontier）
企业级平台必须解决“生成式AI特有的安全和隐私挑战”——哪怕只运行一个智能体也不例外。智能体本身会成为“新的攻击向量”：恶意攻击者可能尝试“提示注入”劫持智能体指令，或“数据投毒”破坏智能体的训练数据或RAG知识库；而且，如果智能体的限制没做好，还可能“不小心泄露敏感客户数据或企业机密”。  

一个稳健的平台会用“深度防御策略”应对这些风险：  
- 从数据入手：确保企业的专有信息“不会被用来训练基础模型”，并通过“VPC服务控制”等措施保护数据。  
- 输入输出过滤：像“防火墙”一样筛查提示和响应。  
- 合同保障：提供“训练数据和生成内容的知识产权保障”，让企业在“生产环境部署智能体”时，既有法律信心，也有技术信心。  


### 智能体治理：用“控制平面”替代“无序扩张”（Agent Governance: A Control Plane instead of Sprawl）
当智能体和工具在企业内扩散时，会形成“复杂的交互网络和潜在漏洞”——这就是常说的“智能体泛滥（agent sprawl）”。要解决这个问题，不能只“保护单个智能体”，而要采用“更高层级的架构方案”：搭建“中央网关”，作为“所有智能体活动的控制平面”。  

可以把这个场景想象成“繁忙的城市”——成千上万的自主车辆（用户、智能体、工具）都在有目的地移动。如果没有红绿灯、车牌和中央控制系统，肯定会乱套。“网关方案”就是要建立这样的控制系统：为所有“智能体相关流量”设定“必经入口”，包括“用户到智能体的提示或UI交互”“智能体到工具的调用（通过MCP）”“智能体到智能体的协作（通过A2A）”“直接调用语言模型的推理请求”。通过掌控这个“关键交汇点”，企业能“监控、路由、管理所有交互”。  

这个控制平面主要有两个相互关联的功能：  

#### 1. 运行时策略执行（Runtime Policy Enforcement）
它是“落实安全策略的架构关卡”，负责处理“认证（这个行动者是谁？）”和“授权（它有权这么做吗？）”。通过“集中执行”，能实现“统一可视化监控”——为所有交易生成通用日志、指标和追踪记录。这能把“杂乱无章的智能体和工作流”，变成“透明、可审计的系统”。  

#### 2. 集中治理（Centralized Governance）
要有效执行策略，网关需要“单一事实来源”——这由“中央注册表”提供，相当于“企业智能体和工具应用商店”。开发者能通过注册表“发现和复用现有资产”，避免重复劳动；管理员则能“掌握完整 inventory（资产清单）”。更重要的是，注册表能“为智能体和工具建立正式生命周期”：发布前做安全审查、版本管理、制定“精细化策略”（规定哪些业务部门能访问哪些智能体）。  

把“运行时网关”和“中央治理注册表”结合起来，企业就能把“无序扩张的风险”，转变成“可管理、安全、高效的生态”。  


### 成本与可靠性：基础设施基石（Cost and Reliability: The Infrastructure Foundation）
说到底，企业级智能体必须“既可靠又经济”。如果智能体经常故障或响应慢，投资回报率（ROI）会是负的；反过来，如果智能体太贵，也没法规模化满足业务需求。底层基础设施的设计，必须平衡这两点——同时确保安全，符合监管要求和数据主权规定。  

有些场景需要“零扩展（scale-to-zero）”能力——比如某个智能体或子功能的流量不稳定。而对于“关键任务、低延迟需求的工作负载”，平台必须提供“专用、有保障的容量”，比如语言模型服务的“预置吞吐量（Provisioned Throughput）”⁴¹，或Cloud Run这类运行时的“99.9%服务级别协议（SLA）”⁴²。这能确保“性能可预测”——哪怕在高负载下，最重要的智能体也能保持响应。  

通过提供“多样化的基础设施选项”，再加上“全面的成本和性能监控”，就能为“智能体从‘有潜力的创新’升级为‘企业核心可靠组件’”，打下最后一块关键基石。  


## 智能体如何进化与学习
部署在现实世界中的智能体，所处的环境是动态变化的——政策、技术、数据格式都在变。如果没有适应能力，智能体的性能会逐渐下降（这个过程常被称为“老化”），最终失去实用性和用户信任。而“手动更新大量智能体”来跟上变化，既不经济又慢。更具规模化的方案，是“设计能自主学习和进化的智能体”——让它们在工作中“自我提升质量”，不需要太多工程师干预⁴³。  


### 智能体的学习与自进化机制
和人类很像，智能体也会“从经验和外部信号中学习”。学习过程的“燃料”来自几类信息：  

#### 1. 运行时经验（Runtime Experience）
智能体会从“运行时产物”中学习，比如会话日志、追踪记录、记忆——这些内容会捕捉“成功案例、失败情况、工具交互、决策轨迹”。其中很关键的是“人机协同（HITL）反馈”——能提供“权威的修正和指导”。  

#### 2. 外部信号（External Signals）
学习也会受“外部新文档”驱动，比如更新后的企业政策、公开监管指南，或其他智能体的“评估意见”。  

这些信息会被用来“优化智能体未来的行为”。高级系统不会只“总结过去的交互”，而是会“创建可泛化的产物”，指导未来任务。目前最成功的“适应技术”主要分两类：  

#### 1. 增强型上下文工程（Enhanced Context Engineering）
系统会“持续优化提示语、少量示例（few-shot examples），以及从记忆中提取的信息”。通过优化“每次任务给语言模型的上下文”，提高“成功概率”。  

#### 2. 工具优化与创建（Tool Optimization and Creation）
智能体的推理能力，能“发现自身能力缺口”，并采取行动填补：比如获取新工具的访问权限、实时创建新工具（如Python脚本），或修改现有工具（如更新API协议）。  

其他优化技术，比如“动态重构多智能体设计模式”“基于人类反馈的强化学习（RLHF）”，目前还在积极研究中。  


### 案例：学习新的合规指南
以“受严格监管行业（如金融或生命科学）的企业智能体”为例。它的任务是“生成符合隐私和监管规则（如GDPR）的报告”。这个场景可以用“多智能体工作流”实现：  

1. **查询智能体（Querying Agent）**：根据用户请求，提取原始数据。  
2. **报告智能体（Reporting Agent）**：把数据整合成报告草稿。  
3. **评估智能体（Critiquing Agent）**：根据“已知合规指南”审查报告。如果遇到模糊内容，或需要最终签字确认，会“上报给人类领域专家”。  
4. **学习智能体（Learning Agent）**：观察整个交互过程，重点关注“人类专家的修正反馈”，然后“把反馈泛化成可复用的新指南”——比如给“评估智能体更新规则”，或给“报告智能体优化上下文”。  

（图7：合规指南相关的多智能体工作流——用户输入需求后，经协调器分配给“查询分解智能体”“查询智能体”“报告智能体”“评估智能体”，同时“学习智能体”会结合人工反馈和更新的合规政策优化系统）  

举个具体例子：如果人类专家指出“某些家庭统计数据必须匿名化”，学习智能体就会记录这个修正。下次生成类似报告时，评估智能体就会“自动应用这个新规则”，减少对人工干预的需求。这种“评估→人工反馈→泛化”的循环，能让系统“自主适应不断变化的合规要求”⁴⁴。  


## 模拟与智能体训练环境（Agent Gym）：下一个前沿领域
我们前面讲的“设计模式”，可以归为“在线学习（in-line learning）”——智能体只能“用搭建时配备的资源和设计模式”学习。目前研究人员正在探索更高级的方案：搭建“专用平台”，通过“离线流程”优化多智能体系统——平台会配备“高级工具和能力”，但这些工具和能力“不属于多智能体的运行时环境”。这类“智能体训练环境（Agent Gym）”⁴⁵ 有几个关键特征：  

1. **独立于执行路径**：是“脱离生产环境的独立平台”，因此可以使用“任何语言模型、离线工具、云应用”等资源。  
2. **提供模拟环境**：智能体能在里面“练习新数据、学习新技能”。这个模拟环境非常适合“试错”，支持多种优化路径。  
3. **可调用高级合成数据生成器**：能“引导模拟贴近真实场景”，还能“压力测试智能体”——比如用“红队测试（redteaming）、动态评估、多评估智能体协作”等高级技术。  
4. **优化工具库可扩展**：优化工具不是固定的，可以通过“MCP或A2A等开放协议”接入新工具；更高级的场景下，还能“学习新概念，围绕概念打造新工具”。  
5. **可对接人类专家**：有些边缘案例，哪怕是“智能体训练环境（Agent Gym）”也解决不了（比如企业中常见的“隐性知识（tribal knowledge）”问题）。这时，训练环境能“对接领域专家网络”，咨询“正确的目标结果”，指导下一步优化。  


## 高级智能体案例
### 谷歌协同科学家（Google Co-Scientist）
“协同科学家（Co-Scientist）”是一款高级AI智能体，定位是“虚拟研究伙伴”——通过“系统探索复杂问题领域”，加速科学发现。研究人员可以“定义目标”，给智能体“指定公开或专有知识库”，然后让它“生成并评估大量新假设”。  

要实现这个目标，协同科学家会“构建一整套智能体生态系统”，让它们协同工作。  

（图8：AI协同科学家设计系统——包含“科学家输入”“文献探索智能体”“生成智能体”“反思智能体”“评估智能体”“排名智能体”等模块）  

可以把这个系统理解成“研究项目经理”：AI先接收“宽泛的研究目标”，制定详细的项目计划；然后“主管智能体（Supervisor）”会扮演“经理”角色，给“专业智能体团队”分配任务，调配“计算资源”等。这种架构能让“项目轻松扩展规模”，还能让“团队方法随目标推进不断优化”。  

（图9：协同科学家多智能体工作流——用户输入研究目标后，主管智能体制定计划，分配给“知识智能体”“生成智能体”“评估智能体”等，最终输出“排名后的想法列表”）  

这些智能体会“连续工作数小时甚至数天”，不断优化“生成的假设”——会运行“多轮循环和元循环（meta loops）”，不仅改进“生成的想法”，还会优化“判断和创建新想法的方式”。  


### AlphaEvolve智能体
另一款高级智能体系统是“AlphaEvolve”——它能“为数学和计算机科学中的复杂问题，发现并优化算法”。  

AlphaEvolve的工作原理，是“结合Gemini语言模型的‘创意代码生成能力’和‘自动化评估系统’”。它采用“进化式流程”：AI生成潜在解决方案→评估器打分→用“得分最高的方案”启发“下一代代码”。  

这种方法已经带来了多项重大突破，比如：  
- 提升谷歌数据中心、芯片设计和AI训练的效率；  
- 发现“更快的矩阵乘法算法”；  
- 为“未解决的数学问题”找到新解法。  

AlphaEvolve特别擅长处理“验证解决方案质量容易、但找到解决方案难”的问题。  

（图10：AlphaEvolve设计系统——人类设定评估标准后，通过“提示采样器”“LLM集成”“分布式控制循环”“评估器池”等模块，最终输出“带质量评分的程序”）  

AlphaEvolve的设计理念是“人类与AI深度、迭代式协作”。这种协作主要通过两种方式实现：  

#### 1. 透明的解决方案（Transparent Solutions）
AI会“以人类可读的代码形式”生成解决方案。这种透明性能让用户“理解逻辑、获得洞察、信任结果”，还能“直接修改代码”满足自身需求。  

#### 2. 专家指导（Expert Guidance）
人类专家的作用至关重要——负责“定义问题”。用户会通过“优化评估指标”“引导探索方向”来指导AI，避免系统“利用问题定义中的漏洞走捷径”。这种“交互循环”能确保“最终解决方案既强大又实用”。  

AlphaEvolve的最终成果，是“代码的持续改进”——会不断优化“人类设定的指标”。  

（图11：算法进化过程——迭代10次后，算法性能显著提升）  


## 结论
生成式AI智能体标志着“人工智能的关键进化”——从“被动的内容生成工具”，转向“主动、自主的问题解决伙伴”。本文为“理解和搭建这类系统”提供了正式框架，帮助大家“超越原型阶段，建立可靠的生产级架构”。  

我们把智能体拆解为三个核心组件：负责推理的“模型（大脑）”、可执行操作的“工具（双手）”、以及统筹管理的“编排层（神经系统）”。正是这三个组件的“无缝集成”，再加上“思考→行动→观察”的持续循环，才释放了智能体的真正潜力。通过“给智能体分等级”——从1级“联网问题解决型”到3级“协作式多智能体系统”——架构师和产品负责人能“根据任务复杂度，合理规划目标”。  

核心挑战（也是机遇）在于“新的开发者范式”。我们不再是“定义明确逻辑的‘泥瓦匠’”，而是“架构师”和“导演”——需要引导、约束和调试“自主实体”。语言模型的“灵活性”既是它的强大之处，也是它“不可靠”的根源。因此，成功的关键不只是“最初的提示语”，更在于“对整个系统的工程严谨性”：包括稳健的工具协议、可靠的错误处理、复杂的上下文管理，以及全面的评估。  

本文阐述的原则和架构模式，是“基础性蓝图”。它们能作为“导航AI新领域的路标”，帮助我们搭建“不只是‘工作流自动化工具’，而是真正‘协作、有能力、适应性强’的团队新成员”。随着这项技术的成熟，这种“严谨的架构方法”，将成为“充分发挥智能体AI潜力”的决定性因素。  


## 附注
1. Julia Wiesinger, Patrick Marlow 等，2024 年，《智能体（Agents）》，获取地址：https://www.kaggle.com/whitepaper-agents。  
2. Antonio Gulli, Lavi Nigam 等，2025 年，《智能体配套指南（Agents Companion）》，获取地址：https://www.kaggle.com/whitepaper-agent-companion。  
3. Shunyu Yao, Y. 等，2022 年，《ReAct：语言模型中推理与行动的协同（ReAct: Synergizing Reasoning and Acting in Language Models）》，获取地址：https://arxiv.org/abs/2210.03629。  
4. Wei, J., Wang, X. 等，2023 年，《思维链提示：激发大语言模型的推理能力（Chain-of-Thought Prompting Elicits Reasoning in Large Language Models）》，获取地址：https://arxiv.org/pdf/2201.11903.pdf。  
5. 同注3。  
6. 参考书籍：《智能体系统设计模式（Agentic System Design Patterns）》，亚马逊链接：https://www.amazon.com/Agentic-Design-Patterns-Hands-Intelligent/dp/3032014018。  
7. Shunyu Yao 等，2024 年，《τ-bench：真实场景下工具-智能体-用户交互基准（τ-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains）》，获取地址：https://arxiv.org/abs/2406.12045。  
8. 参考指南：https://artificialanalysis.ai/guide。  
9. 参考文档：https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/vertex-ai-model-optimizer。  
10. 参考链接：https://gemini.google/overview/gemini-live/。  
11. 参考链接：https://cloud.google.com/vision?e=48754805&hl=en。  
12. 参考链接：https://cloud.google.com/speech-to-text?e=48754