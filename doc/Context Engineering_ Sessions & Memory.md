# 上下文工程：对话会话与记忆
作者：金伯利·米拉姆（Kimberly Milam）、安东尼奥·古利（Antonio Gulli）  
谷歌公司  

## 致谢
### 内容贡献者
凯特琳·阿迪夫（Kaitlin Ardiff）、尚杰·陈（Shangjie Chen）、燕飞·陈（Yanfei Chen）、德里克·伊根（Derek Egan）、林航飞（Hangfei Lin）、伊万·纳尔迪尼（Ivan Nardini）、阿南特·纳瓦尔加里亚（Anant Nawalgaria）、坎查娜·帕特洛拉（Kanchana Patlolla）、黄霞（Huang Xia）、严君（Jun Yan）、杨波（Bo Yang）、迈克尔·齐默尔曼（Michael Zimmermann）  

### 整理与编辑
阿南特·纳瓦尔加里亚（Anant Nawalgaria）、坎查娜·帕特洛拉（Kanchana Patlolla）  
设计：迈克尔·兰宁（Michael Lanning）  

2025年11月  


# 目录
1. 引言 - 第6页  
2. 上下文工程 - 第7页  
3. 对话会话（Sessions） - 第12页  
   3.1 不同框架与模型的差异 - 第13页  
   3.2 多智能体系统的会话 - 第15页  
   3.3 跨多智能体框架的互操作性 - 第19页  
   3.4 会话的生产环境考量 - 第20页  
4. 长上下文对话管理：权衡与优化 - 第22页  
5. 记忆（Memory） - 第27页  
   5.1 记忆的类型 - 第34页  
   5.2 信息的类型 - 第35页  
   5.3 组织模式 - 第35页  
   5.4 存储架构 - 第36页  
   5.5 记忆的范围 - 第37页  
   5.6 生成机制 - 第38页  
   5.7 多模态记忆 - 第39页  
6. 记忆生成：提取与整合 - 第41页  
   6.1 深入解析：记忆提取 - 第44页  
   6.2 深入解析：记忆整合 - 第47页  
7. 记忆溯源 - 第49页  
   7.1 记忆管理中的溯源追踪 - 第50页  
   7.2 推理过程中的溯源考量 - 第52页  
8. 触发记忆生成 - 第52页  
9. 工具化记忆 - 第53页  
10. 后台操作与阻塞操作 - 第56页  
11. 记忆检索 - 第56页  
    11.1 检索时机 - 第58页  
12. 对话历史中的记忆 - 第61页  
13. 基于记忆的推理 - 第61页  
    13.1 系统指令中的记忆 - 第63页  
14. 测试与评估 - 第64页  
15. 隐私与安全风险 - 第65页  
16. 记忆的生产环境考量 - 第67页  
17. 结论 - 第69页  
18. 尾注 - 第70页  


# 上下文工程：对话会话与记忆
具备状态性与个性化的人工智能，始于上下文工程。

## 引言
本白皮书探讨了“对话会话（Sessions）”与“记忆（Memory）”在构建具备状态性的智能大语言模型（LLM）智能体中的核心作用，旨在帮助开发者打造功能更强、更个性化、更具持续性的AI交互体验。要让大语言模型能够“记住信息、学习知识、个性化交互”，开发者必须在模型的“上下文窗口”内动态整合并管理信息——这个过程就是“上下文工程（Context Engineering）”。

下文将简要介绍其中的核心概念：
- **上下文工程**：在大语言模型的上下文窗口内动态整合并管理信息，以构建具备状态性的智能智能体的过程。
- **对话会话**：存储与智能体完整对话的“容器”，记录对话的时间线历史与智能体的工作记忆。
- **记忆**：实现长期信息持久化的机制，跨多个对话会话捕捉并整合关键信息，为大语言模型智能体提供连贯、个性化的交互体验。

2025年11月  


# 上下文工程
大语言模型本质上是“无状态”的——除了训练数据外，它的推理与认知能力仅限于单次API调用中“上下文窗口”内提供的信息。这就带来一个核心问题：AI智能体需要“操作指令”（明确可执行的动作）、“证据与事实数据”（支撑推理的依据），以及“即时对话信息”（定义当前任务的内容）才能正常工作。要构建能“记、学、个性化”的状态性智能体，开发者必须为对话的每一轮交互构建这样的上下文。这种为大语言模型动态整合、管理信息的过程，就是“上下文工程”。

上下文工程是对传统“提示词工程（Prompt Engineering）”的升级。提示词工程侧重设计“最优且通常固定”的系统指令；而上下文工程则关注“完整的信息载荷”，会根据用户、对话历史和外部数据，动态构建“感知状态”的提示词。它需要有策略地筛选、总结、注入各类信息，在最大化相关性的同时减少冗余干扰。RAG数据库、会话存储、记忆管理器等外部系统会负责大部分上下文的管理，而智能体框架则需要协调这些系统，将上下文检索、整合为最终的提示词。

可以把上下文工程比作智能体的“备菜环节”——就像厨师做菜前要把所有食材准备好一样。如果只给厨师一份菜谱（对应提示词），他可能只能用随手找到的食材做出一份普通的菜；但如果先帮他备齐优质食材、专用工具，还明确菜品的呈现要求，他就能稳定做出既美味又贴合需求的定制化菜肴。上下文工程的目标，就是让模型拿到“不多不少、刚好够用”的关键信息，以完成当前任务。

上下文工程所整合的“复杂信息载荷”包含以下几类核心组件：

### 1. 引导推理的上下文
定义智能体的核心推理模式与可执行动作，决定其行为方式：
- **系统指令**：高层级指令，明确智能体的角色定位、能力范围与约束条件。
- **工具定义**：智能体可调用的API或函数的格式说明（用于与外部世界交互）。
- **少量示例（Few-Shot Examples）**：精心挑选的案例，通过“上下文学习”引导模型的推理过程。

### 2. 证据与事实数据
智能体用于推理的核心数据，包括已有知识和为当前任务动态检索的信息，是智能体回复的“依据”：
- **长期记忆**：跨多个对话会话收集的、关于用户或主题的持久化知识。
- **外部知识**：从数据库或文档中检索的信息（通常通过检索增强生成（RAG）技术获取）。
- **工具输出**：工具调用返回的数据或结果。
- **子智能体输出**：被委派处理特定子任务的专业智能体，返回的结论或结果。
- **工件（Artifacts）**：与用户或会话相关的非文本数据（如文件、图片）。

### 3. 即时对话信息
让智能体“锚定”当前交互，明确即时任务：
- **对话历史**：当前交互的逐轮记录。
- **状态/草稿区（State/Scratchpad）**：智能体用于即时推理的临时、待完成信息或计算过程。
- **用户提示**：用户当前需要解决的查询。

上下文的动态构建至关重要。比如“记忆”并非固定不变——随着用户与智能体交互，或新数据的引入，记忆需要被“选择性检索”和“更新”。此外，有效的推理往往依赖“上下文学习”（模型通过提示词中的案例学会完成任务），而如果智能体使用的“少量示例”与当前任务相关（而非硬编码的固定案例），上下文学习的效果会更好。同理，外部知识也需要RAG工具根据用户的即时查询来检索。

构建“感知上下文”的智能体时，一个核心挑战是“对话历史不断变长”。理论上，大上下文窗口的模型能处理长对话记录，但实际中，上下文越长，成本和延迟会越高；而且模型还可能出现“上下文衰减（Context Rot）”——随着上下文内容增加，模型对关键信息的关注度会下降。上下文工程通过“动态处理历史”（如总结、选择性删减、压缩等技术）来解决这个问题：在控制总token数的同时保留关键信息，最终实现更稳定、更个性化的AI体验。

在智能体的每一轮对话循环中，上下文工程体现为一个持续的流程：

（图1：智能体的上下文管理流程）
```
上下文存储（异步）
      ↑
      │ 上传上下文（事件）
      │
用户查询 → 智能体 → 检索上下文 → 准备上下文 → 调用LLM与工具 → 生成回复（流式输出）
                          ↓
                  智能体“决定”检索上下文
```

这个流程分为四步：
1. **检索上下文**：智能体首先获取上下文（如用户记忆、RAG文档、近期对话事件）。对于动态检索，智能体会根据用户查询和其他元数据，确定需要获取哪些信息。
2. **准备上下文**：智能体框架动态构建大语言模型调用所需的完整提示词。虽然单个API调用可能是异步的，但“准备上下文”是“阻塞式”的核心流程——必须等上下文准备好，智能体才能继续。
3. **调用LLM与工具**：智能体反复调用大语言模型和必要工具，直到生成给用户的最终回复。工具和模型的输出会追加到上下文中。
4. **上传上下文**：本轮交互中收集的新信息会上传到持久化存储。这一步通常是“后台流程”——智能体可以先完成回复，而记忆整合等后续处理则异步进行。

这个生命周期的核心是两个组件：对话会话（Sessions）和记忆（Memory）。对话会话管理单次对话的逐轮状态；而记忆则实现长期信息持久化，跨多个会话捕捉并整合关键信息。

可以这样类比：对话会话就像你做某个特定项目时用的工作台或书桌——工作时，桌上摆满所需的工具、笔记和参考资料，所有东西都触手可及，但它们都是临时的，只针对当前任务。项目完成后，你不会把乱糟糟的桌子直接塞进仓库，而是会整理出“记忆”——就像整理文件柜：你会筛选桌上的资料，扔掉草稿和重复笔记，只把最关键、最终版的文件归档到贴好标签的文件夹里。这样，文件柜就能保持整洁、可靠，成为未来所有项目的“可信信息源”，而不会被工作台上的临时杂物堆满。这个类比正好对应智能体的工作方式：对话会话是单次对话的“临时工作台”，而智能体的记忆则是精心整理的“文件柜”，能在未来交互中调用关键信息。

了解了上下文工程的大致情况后，接下来我们会深入探讨两个核心组件：对话会话和记忆，先从对话会话开始。

2025年11月  


# 对话会话（Sessions）
对话会话是上下文工程的基础组件，它封装了“单次连续对话”的即时历史和工作记忆。每个会话都是独立的记录，绑定到特定用户；借助会话，智能体能在单次对话中保持上下文连贯，给出连贯回复。一个用户可以有多个会话，但每个会话都是“独立的”——记录的是某一次特定交互的内容。每个会话包含两个核心部分：时间线历史（事件）和智能体的工作记忆（状态）。

“事件（Events）”是对话的基本单元，常见类型包括：
- 用户输入（用户发送的信息，如文本、音频、图片等）
- 智能体回复（智能体对用户的回应）
- 工具调用（智能体决定使用外部工具或API的操作）
- 工具输出（工具调用返回的数据，供智能体继续推理）

除了聊天历史，会话通常还包含“状态（State）”——一种结构化的“工作记忆”或“草稿区”，存储与当前对话相关的临时结构化数据（比如购物车中的商品）。

随着对话推进，智能体会把新事件追加到会话中，也可能根据自身逻辑修改“状态”。

事件的结构，类似于调用Gemini API时传递的“Content对象列表”——每个带有“角色（role）”和“内容片段（parts）”的对象，就对应对话中的一个“事件”（一轮交互）。

（代码片段1：调用Gemini的多轮对话示例）
```python
contents = [
    {
        "role": "user",  # 用户角色
        "parts": [{"text": "法国的首都是哪里？"}]  # 用户输入内容
    },
    {
        "role": "model",  # 模型角色（智能体）
        "parts": [{"text": "法国的首都是巴黎。"}]  # 智能体回复内容
    }
]

# 调用Gemini模型
response = client.models.generate_content(
    model="gemini-2.5-flash",  # 模型名称
    contents=contents  # 对话内容列表
)
```

生产环境中，智能体的运行环境通常是“无状态”的——处理完一个请求后，不会保留任何信息。因此，必须把对话历史保存到“持久化存储”中，才能保证用户体验的连贯性。开发阶段可以用内存存储，但生产环境需要用可靠的数据库来管理会话（比如用“Agent Engine Sessions”这样的托管方案）。

## 不同框架与模型的差异
虽然核心思路类似，但不同的智能体框架对“会话、事件、状态”的实现方式各不相同。智能体框架的作用是：为大语言模型维护对话历史和状态、用这些上下文构建大语言模型请求、解析并存储大语言模型的回复。

简单说，智能体框架就像“代码与大语言模型之间的通用翻译官”。开发者只需基于框架的统一内部数据结构处理每轮对话，框架会自动把这些结构转换成大语言模型要求的格式。这种“抽象层”的好处是：智能体的逻辑与具体使用的大语言模型解耦，避免被单一厂商绑定。

（图2：智能体的上下文管理流程）
```
用户查询 → 其他数据源
                ↓
智能体框架 → 转换格式 → 模型服务（如Gemini）
                ↑
                ↓
会话存储 ← 转换格式 ← 记忆管理器
（如Agent Engine Sessions）    （如ADK、LangGraph）
```

最终目标是生成大语言模型能理解的“请求”。以谷歌的Gemini模型为例，请求格式是“List[Content]”——每个Content对象是类似字典的结构，包含两个键：
- `role`：标识发言角色（如“user”用户、“model”模型）
- `parts`：存储消息内容（文本、图片、工具调用等）

框架会自动把内部对象（比如ADK的Event对象）映射成Content对象的`role`和`parts`，再调用API。也就是说，框架给开发者提供了稳定的内部API，同时默默处理不同大语言模型API的差异。

举两个框架的例子：
- **ADK框架**：有明确的“Session对象”，包含“Event对象列表”（对话历史）和独立的“State对象”（工作记忆）。可以把它想象成一个文件柜——一个文件夹放对话历史，另一个放工作记忆。
- **LangGraph框架**：没有正式的“Session对象”，而是用“State对象”直接代表会话。这个State对象是“全能的”——既包含对话历史（Message对象列表），也包含所有工作数据。而且它不是“只能追加”的日志，而是可以修改的（比如用“历史压缩”技术删减内容），很适合处理长对话和控制token数。

## 多智能体系统的会话
多智能体系统中，多个智能体会协作完成任务（每个智能体专注于某个细分任务）。要让它们高效协作，必须能共享信息——系统架构会定义它们的通信模式，而“会话历史”（整个交互的完整持久记录）的处理方式是架构的核心部分。

在讨论“会话历史的管理架构”前，需要先区分两个概念：“会话历史”和“传给大语言模型的上下文”。
- 会话历史：整个对话的“完整、未精简”记录，类似会议的逐字稿。
- 上下文：为单次交互传给大语言模型的“精心筛选后的信息载荷”——智能体可能会从历史中截取相关片段，或添加引导性前言，来引导模型的回复。

本节关注的是“智能体之间传递哪些信息”，而非“传给大语言模型的上下文是什么”。

（图3：多智能体系统的不同架构模式）
| 单一智能体       | 网络型架构       | 监督型架构       |
|------------------|------------------|------------------|
| 大语言模型 + 工具 | （如分层架构）   | （如定制化架构） |
| 监督者（作为工具）|                  |                  |
| 大语言模型       |                  |                  |

智能体框架处理多智能体系统的会话历史时，主要有两种模式：
### 模式1：共享统一历史
所有智能体都读写同一个对话历史——每个智能体的消息、工具调用、观察结果，都会按时间顺序追加到一个“中央日志”中。这种模式适合“强耦合的协作任务”（需要单一信息源），比如多步骤问题解决（前一个智能体的输出是后一个的输入）。

即使是共享历史，子智能体在把历史传给大语言模型前，也可能会先处理一下——比如筛选相关事件，或给每个事件添加“所属智能体”的标签。

以ADK框架的“大语言模型驱动委派”为例：子智能体的所有中间事件，都会写到根智能体的同一个会话中。

（代码片段2：跨多智能体框架的A2A通信）
```python
from google.adk.agents import LlmAgent

# 子智能体1：可以访问会话，并向会话中写入事件
sub_agent_1 = LlmAgent(...)

# 子智能体2：可选配置——把最终回复（或结构化输出）保存到指定的状态键中
sub_agent_2 = LlmAgent(
    output_key="..."  # 状态中存储输出的键名
)

# 根智能体（父智能体）：包含两个子智能体
root_agent = LlmAgent(
    sub_agents=[sub_agent_1, sub_agent_2]  # 子智能体列表
)
```

### 模式2：独立私有历史
每个智能体都有自己的“私有对话历史”，对其他智能体来说像“黑盒”——内部过程（比如中间思考、工具使用、推理步骤）都不会对外暴露，只有“最终输出”会通过明确的消息传递给其他智能体。

这种模式通常通过两种方式实现：
- **智能体即工具（Agent-as-a-Tool）**：一个智能体调用另一个智能体时，就像调用普通工具——传递输入，接收完整的最终输出。
- **智能体到智能体协议（A2A Protocol）**：智能体通过结构化协议直接通信。

下一部分会更详细地介绍A2A协议。

## 跨多智能体框架的互操作性
（图4：跨不同框架的多智能体A2A通信）
```
智能体 ←→ A2A协议 ←→ 智能体
（本地智能体）          （本地智能体）
      ↓                  ↓
Vertex AI（Gemini API、第三方模型）
      ↓                  ↓
会话存储 ←→ 智能体开发工具包（ADK）
      ↓                  ↓
检查点存储 ←→ LangGraph框架
      ↓                  ↓
MCP（API与企业服务）
```

智能体框架的“内部数据表示”，给多智能体系统带来了一个关键的架构权衡：框架虽然让智能体与大语言模型解耦，但也让它与“其他框架的智能体”隔离开来——这种隔离在“持久化层”表现得更明显：会话的存储模型通常会把数据库 schema 与框架的内部对象绑定，导致对话记录的“可移植性很差”。

比如，用LangGraph构建的智能体，无法直接解读用ADK构建的智能体所存储的“Session对象”和“Event对象”——这就导致智能体之间无法顺畅地交接任务。

为了解决这种“隔离”，一种新兴的架构模式是“智能体到智能体（A2A）通信”——它能让智能体之间交换消息，但无法解决“共享丰富上下文状态”的核心问题：每个智能体的会话历史都用框架专属的格式存储，因此A2A消息中包含的会话事件，必须经过“转换层”处理才能用。

更可靠的互操作性方案，是把“共享知识”抽象成“框架无关的数据层”（比如用“记忆（Memory）”）。与“会话存储”（保存Event、Message等框架专属的原始对象）不同，“记忆层”存储的是“经过处理的标准化信息”——比如从对话中提取的总结、实体、事实，通常以字符串或字典格式存储。这种数据结构不绑定任何框架，因此能成为“通用数据层”，让不同框架的智能体共享认知资源，实现真正的协作智能，而无需定制转换工具。

## 会话的生产环境考量
当智能体进入生产环境，会话管理系统需要从“简单日志”升级为“企业级服务”，核心要考虑三个方面：安全与隐私、数据完整性、性能。像“Agent Engine Sessions”这样的托管会话存储，就是专门为满足这些生产需求设计的。

### 1. 安全与隐私
保护会话中的敏感信息是底线要求，核心原则是“严格隔离”：
- 每个会话属于单个用户，系统必须确保“一个用户绝不能访问另一个用户的会话数据”（比如通过访问控制列表ACL实现）。
- 每次请求会话存储时，都要验证请求者是否有权访问该会话（即“身份认证与授权”）。

处理“个人身份信息（PII）”的最佳实践是：**在会话数据写入存储前，先脱敏处理**。这是降低数据泄露风险的关键措施——敏感数据不落地，既能减少泄露后的影响范围，也能更轻松地符合GDPR、CCPA等隐私法规，同时增强用户信任。（比如用“Model Armor”这样的工具做脱敏）

### 2. 数据完整性与生命周期管理
生产系统需要明确“会话数据如何存储、如何维护”的规则：
- **会话不能永久保存**：可以设置“存活时间（TTL）”策略，自动删除长期未活跃的会话——既能控制存储成本，也能减少数据管理负担。这需要配套的“数据留存政策”，明确会话保存多久后归档或永久删除。
- **保证事件顺序**：必须确保会话历史中的事件按“时间顺序”追加——对话记录的时间线不能乱，这是会话完整性的基础。

### 3. 性能与可扩展性
会话数据是“用户交互的核心路径”（hot path），性能至关重要：
- 读写会话历史必须足够快，才能保证用户体验流畅。但智能体运行环境通常是无状态的，每轮交互开始时都要从中央数据库拉取完整会话历史——这会产生网络传输延迟。

要降低延迟，关键是“减少传输的数据量”：
- 优化手段：在把会话历史传给智能体前，先筛选或压缩内容。比如删掉“不再需要的旧工具调用输出”（这些内容对当前对话状态已无意义）。

下一部分会详细介绍几种“压缩历史”的策略，帮助管理长上下文对话。

## 长上下文对话管理：权衡与优化
简单架构中，会话是“用户与智能体对话的不可变日志”。但随着对话变长，token消耗会不断增加——虽然现代大语言模型支持长上下文，但对“延迟敏感”的应用来说，仍有不少限制：

1. **上下文窗口限制**：每个大语言模型都有最大token上限，如果对话历史超过这个限制，API调用会直接失败。
2. **API成本**：大多数大语言模型按token收费——历史越长，每轮交互的成本越高。
3. **延迟（速度）**：传给模型的文本越多，处理时间越长，用户等待回复的时间也会变长。压缩历史能让智能体响应更快。
4. **回复质量**：token数越多，上下文里的“噪音”会增加，而且模型的“自回归误差”会累积，导致回复质量下降。

可以用“旅行者打包行李”来类比长对话管理：
- 行李箱 = 智能体的上下文窗口（容量有限）
- 衣物和物品 = 对话中的信息

如果把所有东西都塞进箱子，箱子会又重又乱，想找东西都难——就像上下文窗口过载时，成本上升、响应变慢；但如果东西装太少，可能会漏掉护照、外套这样的必需品——就像智能体丢失关键上下文，回复变得无关或错误。无论是旅行者还是智能体，关键都不是“装得多”，而是“装得对”。

“压缩策略”的作用就是“缩小长对话历史”——在保证关键上下文不丢失的前提下，把对话精简到模型的上下文窗口内，同时降低成本和延迟。核心问题是：**如何在不丢失有用信息的前提下，删掉会话中的冗余内容？**

常见的压缩策略从简单到复杂，主要有三种：

### 1. 保留最近N轮对话
最简单的策略：只保留对话中最近的N轮交互（滑动窗口），删掉更早的内容。

### 2. 基于token的截断
传给模型前，先统计消息的token数——从最新的消息开始往前算，直到token数达到预设上限（比如4000个token），剩下的旧消息直接截断。

### 3. 递归总结（Recursive Summarization）
用AI生成“旧对话的总结”，替代原始的旧对话。对话变长时，智能体会定期调用大语言模型，把“最早的一批消息”总结成一段文字，然后用这段总结+最新的原始消息，组成新的对话历史。

以ADK框架为例，可以用内置插件来“只保留最近N轮对话”——这种方式不会修改会话存储中的历史事件，只是控制传给模型的上下文长度。

（代码片段3：用ADK截断会话，只保留最近N轮）
```python
from google.adk.apps import App
from google.adk.plugins.context_filter_plugin import ContextFilterPlugin

# 创建ADK应用
app = App(
    name='hello_world_app',  # 应用名称
    root_agent=agent,        # 根智能体
    plugins=[
        # 插件配置：保留最近10轮对话 + 最新的用户查询
        ContextFilterPlugin(num_invocations_to_keep=10),
    ],
)
```

需要注意的是：像“递归总结”这样的复杂压缩策略，虽然能降低成本和延迟，但“生成总结”本身是耗时的操作——因此必须**异步在后台执行，并把结果持久化保存**。“后台执行”能避免用户等待；“持久化”能避免重复计算（减少不必要的成本）。通常，智能体的“记忆管理器”会负责生成和保存这些递归总结，同时还要记录“哪些事件已被纳入总结”——避免把原始的长事件再传给模型，造成冗余。

此外，还需要确定“何时触发压缩”，常见的触发方式有三种：

1. **基于计数的触发**（如token数或轮次阈值）：当对话超过预设阈值（比如token数达到窗口上限的80%，或轮次超过20轮）时，触发压缩。这种方式简单有效，适合大多数场景。

2. **基于时间的触发**：压缩不看对话长度，而看“用户是否活跃”。如果用户超过一段时间（比如15或30分钟）没交互，系统就在后台执行压缩。

3. **基于事件的触发**（如语义/任务完成）：智能体检测到“某个任务、子目标或话题结束”时，触发压缩。比如用户完成“订机票”后，就可以把这部分对话总结压缩。

以ADK框架为例，可以用“EventsCompactionConfig”配置“每N轮对话触发一次LLM总结”：

（代码片段4：用ADK配置会话压缩，基于轮次触发总结）
```python
from google.adk.apps import App
from google.adk.apps.app import EventsCompactionConfig

# 创建ADK应用
app = App(
    name='hello_world_app',  # 应用名称
    root_agent=agent,        # 根智能体
    # 压缩配置：每5轮对话触发一次总结，重叠1轮（避免信息断层）
    events_compaction_config=EventsCompactionConfig(
        compaction_interval=5,  # 每5轮触发一次压缩
        overlap_size=1,         # 总结时包含前1轮的内容（保证连贯性）
    ),
)
```

“记忆生成”的核心能力，就是从“冗长杂乱的数据源”中提取“持久化的知识”。本节介绍的“会话压缩”，就是从对话历史中提取信息的典型场景——它把完整的对话记录提炼成关键事实和总结，删掉对话中的冗余废话。

在“压缩”的基础上，下一部分会更全面地探讨“记忆生成与管理”，包括记忆的创建、存储、检索等环节，以及如何用记忆构建智能体的长期知识。

2025年11月  


# 记忆（Memory）
记忆与会话是“共生关系”：会话是生成记忆的主要数据源，而记忆是管理会话规模的关键策略。“记忆”是从对话或数据源中提取的“有意义信息的快照”——它是浓缩后的表示，保留了关键上下文，能用于未来的交互。通常，记忆会跨多个会话持久化存储，以提供连贯、个性化的体验。

作为“独立的专用服务”，“记忆管理器”是多智能体互操作性的基础——它常用“框架无关的数据结构”（比如简单的字符串、字典），因此不同框架构建的智能体都能连接到同一个记忆存储，形成“共享知识库”。

需要注意：有些框架可能会把“会话”或“原始对话”称为“短期记忆”，但在本白皮书中，“记忆”特指“提取后的信息”，而非“逐轮对话的原始记录”。

存储和检索记忆，是构建“复杂智能智能体”的关键——强大的记忆系统能让基础聊天机器人升级为真正的智能体，主要体现在四个能力上：

### 1. 个性化
最常见的用途：记住用户的偏好、事实和过往交互，以便未来的回复更贴合用户。比如记住用户喜欢的球队、坐飞机偏好的座位，能让体验更贴心、更实用。

### 2. 上下文窗口管理
对话变长时，完整历史可能超过大语言模型的上下文窗口——记忆系统可以通过“总结”或“提取关键事实”来压缩历史，在不丢失上下文的同时，避免每轮都传大量token，从而降低成本和延迟。

### 3. 数据挖掘与洞察
通过分析“多个用户的记忆”（需聚合且保护隐私），可以从冗余信息中提炼有价值的洞察。比如零售行业的聊天机器人，发现很多用户询问某款产品的退货政策，就能及时发现潜在问题。

### 4. 智能体自我改进与适应
智能体可以通过“记录自身表现”来学习——比如保存“哪些策略、工具、推理路径能成功解决问题”的“过程性记忆”，形成“有效解决方案手册”，从而在后续问题中不断调整、提升解决能力。

在AI系统中，“创建、存储、使用记忆”是一个协作过程——从用户到开发者代码，每个环节都有明确角色：

1. **用户**：提供记忆的原始数据源。有些系统中，用户还能直接提供记忆（比如填写表单）。

2. **智能体（开发者逻辑）**：配置“记什么、何时记”，协调调用记忆管理器。简单架构中，开发者可以设置“每次都检索记忆、每次都触发生成记忆”；复杂架构中，开发者可以实现“工具化记忆”——让智能体（通过大语言模型）自己决定何时检索、何时生成记忆。

3. **智能体框架（如ADK、LangGraph）**：提供记忆交互的结构和工具，相当于“管道”。它定义了“开发者如何访问对话历史、如何与记忆管理器交互”，但不负责长期存储；同时还定义了“如何把检索到的记忆塞进上下文窗口”。

4. **会话存储（如Agent Engine Sessions、Spanner、Redis）**：存储会话的逐轮对话。这些原始对话会被传入记忆管理器，用于生成记忆。

5. **记忆管理器（如Agent Engine Memory Bank、Mem0、Zep）**：负责记忆的存储、检索、压缩，是核心服务。它处理记忆的完整生命周期：
   - 提取（从数据源中提炼关键信息）
   - 整合（合并重复实体，避免冲突）
   - 存储（把记忆保存到持久化数据库）
   - 检索（获取相关记忆，为新交互提供上下文）

（图5：会话、记忆与外部知识的信息流向）
```
外部知识库（如RAG数据库）
      ↓（读）
智能体 ←→ 工具 → 工具输出
      ↓（读/写）     ↓
记忆管理器 ←→ 生成记忆 ←→ 大语言模型 → 大语言模型输出
      ↓（读）
会话存储 ←→ 写入事件
      ↑（读）
```

这种“职责分工”能让开发者专注于智能体的核心逻辑，无需搭建复杂的记忆持久化基础设施。需要注意的是：记忆管理器是“主动系统”，而非“被动的向量数据库”——虽然它用“相似性搜索”来检索记忆，但核心价值在于“能随着时间推移，智能地提取、整合、整理记忆”。像“Agent Engine Memory Bank”这样的托管记忆服务，会处理记忆生成和存储的全生命周期，让开发者能聚焦于智能体的核心功能。

记忆常被拿来和“检索增强生成（RAG）”比较，但两者的架构原理不同：RAG处理“静态外部数据”，记忆则处理“动态用户专属上下文”，是互补关系——RAG让智能体成为“事实专家”，记忆让智能体成为“用户专家”。下表详细对比了两者的差异：

（表1：RAG引擎与记忆管理器的对比）
| 维度         | RAG引擎                                  | 记忆管理器                              |
|--------------|------------------------------------------|---------------------------------------|
| 核心目标     | 向上下文注入外部事实知识                  | 提供个性化、有状态的体验——记住事实、适应用户、维护长期上下文 |
| 数据源       | 静态、预索引的外部知识库（如PDF、维基、API） | 用户与智能体的对话                      |
| 隔离级别     | 通常共享——知识库是全局只读资源，所有用户可访问，确保答案一致 | 高度隔离——几乎都是用户专属，防止数据泄露 |
| 信息类型     | 静态、事实性、权威性——常包含领域数据、产品详情、技术文档 | 动态、用户专属——从对话中提取，存在一定不确定性 |
| 写入模式     | 批量处理——通过离线管理员操作触发          | 基于事件处理——按周期（如每轮对话、会话结束）触发，或“工具化记忆”（智能体决定） |
| 读取模式     | 几乎都是“工具化”——智能体判断用户查询需要外部信息时才检索 | 两种常见模式：<br>1. 工具化：用户查询需要用户信息时检索<br>2. 静态检索：每轮对话开始时必检索 |
| 数据格式     | 自然语言“片段”（chunk）                  | 自然语言片段或结构化用户档案            |
| 数据准备     | 分片与索引——把源文档拆成小片段，转成向量后存储，方便快速查询 | 提取与整合——从对话中提取关键信息，确保无重复、无冲突 |

可以用两个角色来理解两者的差异：
- **RAG = 智能体的研究图书馆员**：在装满百科全书、教科书、官方文档的“公共图书馆”工作。当智能体需要“产品技术参数”“历史日期”这样的既定事实时，就找图书馆员——它从“静态、共享、权威”的知识库中检索信息，给出一致的事实答案，但不了解查询用户的个人情况。

- **记忆管理器 = 智能体的私人助理**：随身带着“专属笔记本”，记录与特定用户的每一次交互。这个笔记本是“动态、私密”的，包含用户偏好、过往对话、待办目标。当智能体需要“用户喜欢的球队”“上周项目讨论的内容”时，就找私人助理——它不掌握全局事实，但精通用户本人的情况。

说到底，真正智能的智能体需要两者兼备：RAG提供“世界知识”，记忆提供“用户知识”。

下一部分会拆解“记忆”的核心组件，包括它存储的信息类型、组织模式、存储架构、生成机制、范围定义，以及如何处理多模态与文本数据。

## 记忆的类型
智能体的记忆可以按“存储方式”和“获取方式”分类——不同类型的记忆协同工作，帮助智能体深入理解用户及其需求。需要强调的是：所有类型的记忆都是“描述性的”，而非“预测性的”。

“记忆”是记忆管理器返回的“原子化上下文片段”，供智能体使用。虽然具体格式可能不同，但单个记忆通常包含两个核心部分：内容（content）和元数据（metadata）。

### 1. 内容（Content）
从数据源（如会话的原始对话）中提取的核心信息，关键是“框架无关”——用简单数据结构（所有智能体都能轻松处理）存储。内容分为两种：
- **结构化记忆**：用通用格式（如字典、JSON）存储的信息， schema 由开发者定义（不绑定特定框架）。比如 `{"seat_preference": "Window"}`（座位偏好：靠窗）。
- **非结构化记忆**：用自然语言描述的“核心信息”，捕捉长交互、事件或话题的本质。比如“The user prefers a window seat.”（用户偏好靠窗座位）。

### 2. 元数据（Metadata）
描述记忆本身的上下文信息，通常是简单字符串，包括：
- 记忆的唯一标识（ID）
- 记忆“所有者”的标识（如用户ID）
- 描述记忆内容或数据源的标签（如“2025年11月机票预订对话”）

## 信息的类型
除了基础结构，记忆还可以按“知识类型”分类——这对理解智能体如何使用记忆至关重要。分类源自认知科学，主要分为两类：

### 1. 陈述性记忆（Declarative Memory）：“知道是什么”
智能体掌握的“事实、数据、事件”——所有能明确“陈述”的信息。只要记忆能回答“是什么”的问题，就是陈述性记忆。它又分为：
- **语义记忆（Semantic）**：通用世界知识（如“巴黎是法国首都”）。
- **实体/情景记忆（Entity/Episodic）**：用户专属的特定事实（如“用户的生日是10月5日”）。

### 2. 过程性记忆（Procedural Memory）：“知道怎么做”
智能体掌握的“技能、流程”——指导智能体“如何行动”的隐性知识。只要记忆能回答“怎么做”的问题（比如“订机票需要调用哪些工具、按什么顺序”），就是过程性记忆。

## 组织模式
创建记忆后，下一步是“如何组织”——记忆管理器通常用以下一种或多种模式来组织记忆，定义单个记忆之间、记忆与用户之间的关系：

### 1. 集合模式（Collections）
把单个用户的记忆，组织成多个“独立的自然语言记忆集合”。每个记忆都是一个独立的事件、总结或观察结果——即使是同一个大主题，也可能有多个记忆。这种模式适合存储、搜索“与特定目标或话题相关的大量非结构化信息”。

### 2. 结构化用户档案模式（Structured User Profile）
把记忆组织成“用户核心事实集合”，类似“联系人卡片”——会不断用新的稳定信息更新。适合快速查询“关键事实信息”（如姓名、偏好、账户详情）。

### 3. “滚动”总结模式（Rolling Summary）
把所有信息整合到“单个不断更新的记忆”中——用自然语言总结“用户与智能体的整个交互历史”。不创建新的独立记忆，而是持续更新这一个“主文档”。这种模式常用来压缩长会话，在控制token数的同时保留关键信息。

## 存储架构
存储架构是关键决策——它决定了智能体“检索记忆的速度”和“理解能力”，以及是否擅长“找相似概念”“理解结构化关系”。

记忆通常存储在“向量数据库”和/或“知识图谱”中：

### 1. 向量数据库（Vector Databases）
最常用的方式，支持“基于语义相似性的检索”（而非精确关键词匹配）。记忆会被转换成“嵌入向量（embedding vectors）”，数据库能找到与用户查询“概念最接近”的记忆。适合检索“非结构化自然语言记忆”（如“原子事实”）——重点是上下文和语义。

### 2. 知识图谱（Knowledge Graphs）
把记忆存储成“实体（节点）与关系（边）的网络”。检索时通过“遍历图谱”找到“直接或间接关联”的信息，帮助智能体理解“不同事实之间的联系”。适合“结构化关系查询”（如“知识三元组”）——重点是实体间的关联。

### 3. 混合架构（Hybrid Approach）
也可以结合两种方式：给知识图谱中的结构化实体添加“向量嵌入”。这样既能做“关系推理”（图谱的优势），也能做“精细的概念搜索”（向量数据库的优势），兼顾两者的优点。

## 生成机制
记忆还可以按“创建方式”（即信息的获取途径）分类：

### 1. 按“是否用户直接指令”分
- **显式记忆（Explicit Memories）**：用户直接命令智能体“记住某件事”时创建。比如用户说“记住我的结婚纪念日是10月26日”。
- **隐式记忆（Implicit Memories）**：智能体从对话中“推断、提取”信息时创建，无需用户直接指令。比如用户说“我的结婚纪念日下周，帮我找个礼物”，智能体自动提取“用户近期有结婚纪念日”的信息。

### 2. 按“提取逻辑的位置”分
- **内部记忆（Internal Memory）**：记忆管理功能直接内置在智能体框架中。上手简单，但通常缺乏高级功能。内部记忆也可以用外部存储，但“生成记忆的机制”在智能体内部。
- **外部记忆（External Memory）**：用独立的专用服务管理记忆（如Agent Engine Memory Bank、Mem0、Zep）。智能体框架通过API调用这个外部服务，实现记忆的存储、检索、处理。这种方式有更高级的功能（如语义搜索、实体提取、自动总结），把复杂的记忆管理工作交给“专门工具”。

## 记忆的范围
还需要考虑“记忆描述的对象是谁/是什么”——这决定了“按哪个实体（用户、会话、应用）来聚合、检索记忆”。

### 1. 用户级范围（User-Level Scope）
最常见的实现方式，为“单个用户”提供连贯、个性化的体验。比如“用户偏好中间座位”——记忆绑定到特定用户ID，跨所有会话持久化，帮助智能体长期理解用户的偏好和历史。

### 2. 会话级范围（Session-Level Scope）
用于“压缩长对话”。比如“用户正在预订2025年11月7日至14日纽约到巴黎的机票，偏好直飞和中间座位”——它记录“单次会话中提取的洞察”，能用“简洁的关键事实”替代“冗长、耗token的对话记录”。需要注意：这种记忆与“原始会话日志”不同——只包含处理后的洞察，不包含原始对话，且上下文仅限于该次会话。

### 3. 应用级范围（Application-Level Scope，或全局上下文）
所有应用用户都能访问的记忆。比如“项目代号XYZ指的是……”——用于提供“共享上下文”“系统级通知”或“通用基础知识”。常见用途是存储“过程性记忆”（指导智能体如何工作的“操作手册”），帮助所有用户的交互更顺畅。必须注意：这类记忆要彻底脱敏，避免用户间的数据泄露。

## 多模态记忆
“多模态记忆”是关键概念，描述智能体如何处理“图片、视频、音频”等非文本信息。核心是区分“记忆的数据源”（信息来自哪里）和“记忆的存储形式”（信息以什么格式保存）。

### 1. 从多模态数据源生成的记忆（最常见）
智能体可以处理文本、图片、音频等多种数据，但生成的记忆是“从这些数据源中提取的文本洞察”。比如用户发送一段语音备忘录，智能体不会存储音频文件本身，而是先转文字，再生成文本记忆：“用户对最近的物流延迟表示不满”。

### 2. 包含多模态内容的记忆（更高级）
记忆本身包含“非文本媒体”——智能体不只是“描述内容”，而是“直接存储内容”。比如用户上传一张图片，说“记住这个logo设计”，智能体创建的记忆会直接包含这张图片文件，并关联用户的指令。

当前大多数记忆管理器，都侧重“处理多模态数据源，生成文本记忆”。原因是：要为特定记忆生成、检索“图片、音频”这样的非结构化二进制数据，需要专用的模型、算法和基础设施——而把所有输入转换成“文本”这种通用、可搜索的格式，要简单得多。

以“Agent Engine Memory Bank”为例，可以从多模态输入中生成记忆——输出的记忆是“从内容中提取的文本洞察”：

（代码片段5：用Agent Engine Memory Bank生成多模态记忆的API调用示例）
```python
from google.genai import types
from google.cloud import vertexai

# 初始化Vertex AI客户端
client = vertexai.Client(project="你的项目ID", location="你的区域")

# 调用记忆生成API
response = client.agent_engines.memories.generate(
    name="agent_engine的名称",  # Agent Engine的名称
    # 多模态输入源（事件列表）
    direct_contents_source={
        "events": [
            {
                "content": types.Content(
                    role="user",  # 用户角色
                    parts=[
                        # 文本部分
                        types.Part.from_text("这是多模态输入的上下文说明。"),
                        # 二进制数据部分（如图片）
                        types.Part.from_bytes(
                            data=CONTENT_AS_BYTES,  # 二进制数据（如图片字节流）
                            mime_type=MIME_TYPE     # MIME类型（如image/png）
                        ),
                        # 从URI加载的文件部分（如本地文件）
                        types.Part.from_uri(
                            file_uri="file/path/to/content",  # 文件路径
                            mime_type=MIME_TYPE               # MIME类型
                        )
                    ]
                )
            }
        ]
    },
    scope={"user_id": "用户ID"}  # 记忆的所有者（用户ID）
)
```

下一部分会探讨“记忆生成的机制”，详细拆解两个核心阶段：从数据源中提取新信息，以及将新信息与已有记忆整合。

2025年11月  


# 记忆生成：提取与整合
记忆生成能“自动把原始对话数据转换成结构化、有意义的洞察”，本质上是一个“由大语言模型驱动的ETL流程（提取、转换、加载）”——专门用于提取、压缩记忆。正是这个ETL流程，让记忆管理器区别于RAG引擎和传统数据库。

传统数据库需要开发者手动定义数据库操作，但记忆管理器会用大语言模型“智能判断”何时添加、更新、合并记忆——这种自动化是记忆管理器的核心优势：它帮开发者省去了“管理数据库内容、串联大语言模型调用、部署数据处理后台服务”的复杂工作。

（图6：记忆生成的高层算法——从新数据源提取记忆，并与已有记忆整合）
```
数据源1（如会话事件）→ 原始记忆1 → 与已有相似记忆对比 → 整合（如新增）
                          ↓
数据源2 → 原始记忆2 → 与已有相似记忆对比 → 整合（如更新）
                          ↓
数据源3 → 原始记忆3 → 与已有相似记忆对比 → 整合（如删除）
```

不同平台（如Agent Engine Memory Bank、Mem0、Zep）的具体算法可能不同，但记忆生成的高层流程通常包含四步：

1. **摄入（Ingestion）**：客户端把“原始数据源”（通常是对话历史）传给记忆管理器，流程开始。

2. **提取与筛选（Extraction & Filtering）**：记忆管理器用大语言模型从数据源中“提取有意义的内容”——关键是“不提取所有信息”，只保留“符合预设主题定义”的内容。如果摄入的数据中没有符合主题的信息，就不创建记忆。

3. **整合（Consolidation）**：最复杂的阶段，处理“冲突解决”和“去重”。记忆管理器会用大语言模型做“自我编辑”——把新提取的信息与已有记忆对比，确保用户的知识库“连贯、准确、能随新信息更新”，具体会做以下操作：
   - 把新洞察合并到已有记忆中；
   - 如果已有记忆失效，就删除它；
   - 如果是新主题，就创建全新的记忆。

4. **存储（Storage）**：最后，把“新记忆”或“更新后的记忆”保存到“持久化存储层”（如向量数据库、知识图谱），供未来交互检索。

像“Agent Engine Memory Bank”这样的托管记忆管理器，会完全自动化这个流程——提供“从对话冗余信息到结构化知识”的端到端系统，让开发者能专注于智能体逻辑，无需搭建、维护底层数据基础设施。比如，只需简单调用API，就能触发Memory Bank的记忆生成：

（代码片段6：用Agent Engine Memory Bank生成记忆的Python示例）
```python
from google.cloud import vertexai

# 初始化Vertex AI客户端
client = vertexai.Client(project="你的项目ID", location="你的区域")

# 调用记忆生成API
client.agent_engines.memories.generate(
    name="projects/你的项目ID/locations/你的区域/reasoningEngines/你的引擎名",
    # 记忆的范围（用户ID）
    scope={"user_id": "123"},
    # 直接传入的内容源（事件列表）
    direct_contents_source={"events": [...]},
    # 配置：后台运行记忆生成（不阻塞）
    config={"wait_for_completion": False}
)
```

可以用“园丁打理花园”来类比记忆生成：
- **提取阶段**：收到新的种子和幼苗（对话中的新信息）。
- **整合阶段**：园丁不会随便把种子撒在地里，而是先拔杂草（删除重复或冲突数据）、修剪枯枝（优化、总结已有记忆），再把新幼苗种到合适的位置（添加新记忆）。
- **最终效果**：通过持续的精心打理，花园（记忆库）能保持健康、整洁，不断生长，而不会变成杂乱无章的荒地。而且这个过程是“后台异步”的——确保花园随时能为下一次“访问”（用户交互）做好准备。

接下来，我们深入拆解记忆生成的两个关键步骤：提取与整合。

## 深入解析：记忆提取
记忆提取的核心目标是回答一个问题：“对话中的哪些信息值得被记下来？”——这不是简单的总结，而是“有针对性的智能筛选”，要把“信号（重要事实、偏好、目标）”和“噪音（客套话、冗余废话）”分开。

“有意义”不是通用概念，完全由智能体的“用途”决定：客服智能体需要记住“订单号、技术问题”，而个人健康教练需要记住“用户长期目标、情绪状态”——因此，“定制化提取的信息”是打造高效智能体的关键。

记忆管理器的大语言模型会“遵循精心设计的程序化约束和指令”（通常嵌入在复杂的系统提示词中），来决定提取什么信息。这些指令通过“主题定义”明确“什么是有意义的”，具体有三种方式：

### 1. 基于schema和模板的提取
给大语言模型提供“预定义的JSON schema”或“模板”（利用大语言模型的“结构化输出”功能），让它根据对话中的信息，填充出符合schema的JSON。

### 2. 基于自然语言主题定义的提取
用简单的自然语言描述“需要提取的主题”，引导大语言模型。比如“提取用户提到的所有旅行计划”。

### 3. 基于少量示例的提取（Few-Shot Prompting）
给大语言模型“展示示例”——提供“输入文本”和“理想的提取结果”，让它从示例中学会“该提取什么信息”。这种方式对“难以用schema或简单定义描述的定制化、细致主题”特别有效。

大多数记忆管理器“开箱即用”时，会默认寻找“常见主题”（如用户偏好、关键事实、目标）；很多平台还允许开发者“自定义主题”，让提取过程贴合具体业务场景。

以“Agent Engine Memory Bank”为例，可以通过“自定义主题定义”和“少量示例”，配置它“认为哪些信息值得持久化”：

（代码片段7：定制Agent Engine Memory Bank的记忆提取规则）
```python
from google.genai.types import Content, Part
from google.cloud import vertexai

# 初始化客户端
client = vertexai.Client(project="你的项目ID", location="你的区域")

# 配置记忆库：定义自定义主题和示例
memory_bank_config = {
    "customization_configs": [
        {
            "memory_topics": [
                # 1. 内置主题：用户个人信息
                {"managed_memory_topic": {"managed_topic_enum": "USER_PERSONAL_INFO"}},
                # 2. 自定义主题：用户对咖啡店的反馈
                {
                    "custom_memory_topic": {
                        "label": "business_feedback",  # 主题标签
                        "description": """用户对咖啡店体验的具体反馈，包括对饮品、食物、糕点、环境、服务态度、出餐速度、卫生状况的评价，以及改进建议。""",
                        # 少量示例：告诉模型“该提取什么”
                        "generate_memories_examples": {
                            # 示例对话
                            "conversationSource": {
                                "events": [
                                    # 智能体提问
                                    {
                                        "content": Content(
                                            role="model",
                                            parts=[Part(text="欢迎回到The Daily Grind！能说说你这次的体验吗？")]
                                        )
                                    },
                                    # 用户反馈
                                    {
                                        "content": Content(
                                            role="user",
                                            parts=[Part(text="嗨，今天的滴滤咖啡有点温，不太满意。而且音乐太吵了，我几乎听不见朋友说话。")]
                                        )
                                    }
                                ]
                            },
                            # 示例提取结果（理想的记忆）
                            "generatedMemories": [
                                {"fact": "用户反馈滴滤咖啡温度偏温。"},
                                {"fact": "用户认为店内音乐音量过大。"}
                            ]
                        }
                    }
                }
            ]
        }
    ]
}

# 创建Agent Engine，应用记忆库配置
agent_engine = client.agent_engines.create(
    config={
        "context_spec": {"memory_bank_config": memory_bank_config}
    }
)
```

需要注意：“记忆提取”本身不是“总结”，但算法中可能会融入总结步骤，以提高效率。比如很多记忆管理器会在“记忆提取的提示词”中，直接加入“对话的滚动总结”——这份浓缩的历史能为“提取最新交互的关键信息”提供必要上下文，避免每次都要重新处理完整的冗长对话。

新信息从数据源中提取后，下一步就是“整合”——把新信息更新到已有记忆库中。

## 深入解析：记忆整合
从冗长对话中提取记忆后，需要通过“整合”把新信息融入“连贯、准确、不断更新的知识库”。这是记忆生命周期中最复杂的阶段——它能把“简单的事实集合”变成“精心整理的用户理解”。如果没有整合，智能体的记忆会很快变成“杂乱、矛盾、不可靠的信息日志”。这种“自我整理”通常由大语言模型完成，也是记忆管理器区别于“简单数据库”的关键。

整合主要解决对话数据中的四个核心问题：

### 1. 信息重复
用户可能在不同对话中“用不同方式说同一件事”（比如“我要去纽约”和“我计划去NYC”）——如果只是简单提取，会生成两个重复的记忆。

### 2. 信息冲突
用户的状态会变化（比如之前说“喜欢靠窗座位”，后来改成“喜欢过道座位”）——没有整合的话，记忆库中会同时存在矛盾的事实。

### 3. 信息演进
简单事实会变得更细致（比如最初的记忆是“用户对营销感兴趣”，后来补充为“用户负责Q4客户获取的营销项目”）。

### 4. 记忆相关性衰减
不是所有记忆都永远有用——智能体需要“主动遗忘”：定期删除“过时、不重要、可信度低”的记忆，保持知识库的相关性和效率。实现“遗忘”的方式有两种：一是让大语言模型在整合时“优先采用新信息”；二是给记忆设置“存活时间（TTL）”，到期自动删除。

整合是“由大语言模型驱动的流程”，核心是“把新提取的洞察与用户已有记忆对比”，具体分三步：

第一步：**检索相似已有记忆**  
先找到“与新提取记忆相似的已有记忆”——这些是潜在的整合对象。如果已有记忆与新信息冲突，可能会被删除；如果已有记忆能被补充，可能会被更新。

第二步：**大语言模型判断操作类型**  
把“已有记忆”和“新信息”都传给大语言模型，让它分析后决定“该做什么操作”，主要有三种：
- **更新（UPDATE）**：用新信息修改已有记忆（比如补充细节、修正错误）。
- **创建（CREATE）**：如果新洞察是“全新的，与已有记忆无关”，就创建新记忆。
- **删除/失效（DELETE/INVALIDATE）**：如果新信息让旧记忆“完全无关或错误”，就删除或标记旧记忆失效。

第三步：**执行更新操作**  
记忆管理器把大语言模型的决策，转换成“更新记忆库的事务”（比如写入数据库、删除旧记录）。

2025年11月  


# 记忆溯源（Memory Provenance）
机器学习中有句经典格言：“输入垃圾，输出垃圾（Garbage In, Garbage Out）”——对大语言模型来说，情况更甚：“输入垃圾，输出自信的垃圾（Garbage In, Confident Garbage Out）”。要让智能体做出可靠决策、让记忆管理器有效整合记忆，必须能“批判性评估记忆的质量”——而这种“可信度”直接来自记忆的“溯源（Provenance）”：记录记忆的“来源”和“历史”的详细信息。

（图7：数据源与记忆的信息流向——单个记忆可来自多个数据源，单个数据源也可生成多个记忆）
```
请求1（数据源A）→ 记忆1 → 记忆修订版1
                          ↓
请求2（数据源B）→ 记忆修订版2
                          ↓
请求3（数据源C）→ 记忆修订版3
                          ↓
请求4（数据源D）→ 记忆2 → 记忆修订版1 → 记忆修订版2
```

“记忆整合”会把“多个数据源的信息合并成一个不断更新的记忆”，这就需要追踪记忆的“谱系（Lineage）”——如上图所示：一个记忆可能融合了多个数据源的信息，而一个数据源也可能拆分成多个记忆。

要评估记忆的可信度，智能体需要追踪“每个数据源的关键细节”，比如“来源类型”和“时效性（新鲜度）”——这些细节至关重要，原因有二：
1. 决定“整合时每个数据源的权重”（比如更信任新信息还是旧信息）；
2. 决定“推理时对记忆的依赖程度”（比如更依赖权威来源还是普通来源）。

“来源类型”是判断可信度的最重要因素之一，主要分三类：

### 1. 引导数据（Bootstrapped Data）
从内部系统预加载的信息（如CRM中的客户数据）。这类数据可信度高，常用于“解决冷启动问题”——即“智能体第一次和用户交互时，如何提供个性化体验”（因为此时还没有对话历史）。

### 2. 用户输入
用户直接提供的信息，又分两种：
- 显式输入（如填写表单）：可信度高；
- 从对话中隐式提取的信息：可信度通常较低。

### 3. 工具输出
外部工具调用返回的数据。**不建议用工具输出生成记忆**——因为工具输出通常“不稳定、易过时”，更适合短期缓存，而非长期记忆。

## 记忆管理中的溯源追踪
这种“多源动态构建记忆”的方式，给记忆管理带来两个核心运营挑战：“冲突解决”和“衍生数据删除”。

### 1. 冲突解决
记忆整合过程中，不同数据源的信息难免冲突——记忆的溯源能帮助记忆管理器“建立信息源的信任层级”。当不同来源的记忆冲突时，智能体可以用以下策略解决：
- 优先采用“信任度高的来源”（如引导数据 > 显式用户输入 > 隐式提取 > 工具输出）；
- 优先采用“更新的信息”；
- 寻找“多个数据源的共识”（比如多个来源都提到同一事实，可信度更高）。

### 2. 衍生数据删除
一个记忆可能来自多个数据源——如果用户“撤销某个数据源的访问权限”，那么“从该数据源衍生的记忆”也应该被删除。但“删除所有与该数据源相关的记忆”可能过于激进，更精准（但计算成本更高）的方式是：“用剩余的有效数据源，重新生成受影响的记忆”。

除了“静态的溯源细节”，记忆的“可信度”还需要“动态变化”：
- 可信度提升：如果“多个可信来源都提到同一信息”（即“交叉验证”），可信度会提高；
- 可信度降低：记忆会“随时间衰减”（旧记忆变陈旧），或“出现矛盾信息”时，可信度会下降；
- 最终遗忘：当可信度低到一定阈值，系统可以“归档”或“删除”该记忆。

通过“被动的整合流程”和“主动的记忆修剪”，记忆管理器能确保：智能体的知识库“不是所有对话的流水账”，而是“精心整理、准确、相关的用户理解”。

## 推理过程中的溯源考量
除了“整理记忆库时要考虑溯源”，“推理时也要考虑记忆的可信度”。智能体对记忆的“信任度”不是固定的，而是要“随新信息和时间动态调整”——比如：
- 多个可信来源交叉验证 → 可信度提升；
- 记忆变旧 → 可信度衰减；
- 出现矛盾信息 → 可信度下降；
- 最终，低可信度记忆会被“遗忘”（归档或删除）。

这种“动态可信度分数”在推理时至关重要——但它不会直接展示给用户，而是会“和记忆一起注入提示词”，让大语言模型能“评估信息可靠性”，做出更细致的决策。

整个“信任框架”都是为了“辅助智能体的内部推理”——记忆和可信度分数通常不直接展示给用户，而是嵌入到“系统提示词”中，让大语言模型能“权衡证据、考虑信息可靠性”，最终给出更可信、更细致的回复。

## 触发记忆生成
虽然“一旦触发，记忆管理器会自动完成提取和整合”，但“何时触发记忆生成”仍需智能体来决定——这是关键的架构选择，需要在“数据新鲜度”和“计算成本、延迟”之间权衡。这个决策通常由智能体的逻辑控制，常见的触发策略有四种：

1. **会话结束时触发**：在多轮对话结束后生成记忆。
2. **按轮次触发**：每完成N轮对话（如每5轮）触发一次。
3. **实时触发**：每轮对话后都生成记忆。
4. **显式指令触发**：用户直接命令（如“记住这个”）时触发。

选择哪种触发方式，本质是“成本与准确性的权衡”：
- 高频触发（如实时触发）：记忆更细致、更新鲜，能捕捉对话的每个细节，但大语言模型和数据库的成本最高，若处理不当还会增加延迟；
- 低频触发（如会话结束时触发）：成本低，但记忆的“准确性可能下降”（因为大语言模型需要一次性总结大量对话）。

另外还要注意：**避免让记忆管理器重复处理同一事件**——否则会产生不必要的成本。

## 工具化记忆（Memory-as-a-Tool）
更复杂的方式是“让智能体自己决定何时创建记忆”——把“记忆生成”封装成一个工具（比如`create_memory`函数），工具定义中明确“哪些类型的信息值得记”。智能体分析对话后，若发现“值得持久化的信息”，会自动调用这个工具。这种方式把“判断什么信息有意义”的责任，从“外部记忆管理器”转移到了“智能体（即开发者）”身上。

以ADK框架为例，可以把“记忆生成代码”封装成一个“工具”——智能体认为“对话中有值得记的信息”时，会自动调用这个工具。工具会把“会话数据传给Memory Bank”，由Memory Bank从对话历史中提取、整合记忆：

（代码片段8：ADK智能体用自定义工具触发记忆生成）
```python
from google.adk.agents import LlmAgent
from google.adk.memory import VertexAiMemoryBankService
from google.adk.runners import Runner
from google.adk.tools import ToolContext

# 定义“生成记忆”的工具函数
def generate_memories(tool_context: ToolContext):
    """触发记忆生成，保存当前会话信息。"""
    # 方式1：用ADK的记忆服务，从完整对话历史中提取记忆
    tool_context._invocation_context.memory_service.add_session_to_memory(
        session=tool_context._invocation_context.session
    )

    # 方式2：从最新一轮对话中提取记忆
    client = VertexAiMemoryBankService(
        agent_engine_id=AGENT_ENGINE_ID,
        project=PROJECT,
        location=LOCATION
    )
    client.agent_engines.memories.generate(
        name="projects/你的项目ID/locations/你的区域/reasoningEngines/你的引擎名",
        # 传入最新一轮的用户内容
        direct_contents_source={
            "events": [
                {"content": tool_context._invocation_context.user_content}
            ]
        },
        # 记忆的范围（用户ID、应用名）
        scope={
            "user_id": tool_context._invocation_context.user_id,
            "app_name": tool_context._invocation_context.app_name
        },
        # 配置：后台运行，不阻塞
        config={"wait_for_completion": False}
    )

    return {"status": "success"}  # 返回工具执行结果

# 创建智能体，添加“生成记忆”工具
agent = LlmAgent(
    ...,  # 其他配置
    tools=[generate_memories]  # 把工具加入智能体
)

# 创建运行器，配置会话服务和记忆服务
runner = Runner(
    agent=agent,
    app_name=APP_NAME,
    session_service=session_service,  # 会话服务
    memory_service=VertexAiMemoryBankService(  # 记忆服务
        agent_engine_id=AGENT_ENGINE_ID,
        project=PROJECT,
        location=LOCATION
    )
)
```

另一种方式是“利用内部记忆”——让智能体主动从对话中“提取关键信息”，然后（可选）把这些提取的记忆传给“Agent Engine Memory Bank”，与用户已有记忆整合：

（代码片段9：ADK智能体提取记忆并触发整合）
```python
from google.cloud import vertexai

# 定义“提取记忆”的工具函数
def extract_memories(query: str, tool_context: ToolContext):
    """从对话中提取有意义的信息，触发记忆整合。
    
    参数：
        query: 需要持久化的用户相关信息（已从对话中提取）
    """
    # 初始化Vertex AI客户端
    client = vertexai.Client(project=PROJECT, location=LOCATION)

    # 调用记忆生成API：只做整合，不重新提取
    client.agent_engines.memories.generate(
        name="projects/你的项目ID/locations/你的区域/reasoningEngines/你的引擎名",
        # 直接传入已提取的记忆（避免Memory Bank重复提取）
        direct_memories_source={
            "direct_memories": [{"fact": query}]  # 已提取的事实
        },
        # 记忆的范围（用户ID、应用名）
        scope={
            "user_id": tool_context._invocation_context.user_id,
            "app_name": tool_context._invocation_context.app_name
        },
        # 配置：后台运行
        config={"wait_for_completion": False}
    )

    return {"status": "success"}

# 创建智能体，添加“提取记忆”工具
agent = LlmAgent(
    ...,  # 其他配置
    tools=[extract_memories]  # 把工具加入智能体
)
```

## 后台操作与阻塞操作
“记忆生成”是高成本操作——需要调用大语言模型和写入数据库，因此生产环境中的智能体，**几乎都应该把记忆生成作为“后台异步操作”处理**。

智能体给用户返回回复后，记忆生成流程可以“并行后台运行”——不阻塞用户体验。这种“解耦”是保证智能体“响应迅速”的关键：如果用“阻塞式（同步）”方式（用户要等记忆写完才能收到回复），会造成严重的延迟，体验极差。这就要求“记忆生成”必须在“智能体核心运行时之外的独立服务”中执行。

2025年11月  


# 记忆检索
有了“记忆生成机制”后，重点就变成了“如何检索记忆”——智能的检索策略对智能体性能至关重要，需要决定“检索哪些记忆”和“何时检索”。

检索策略很大程度上取决于“记忆的组织方式”：
- 如果是“结构化用户档案”，检索通常很简单——直接查询“完整档案”或“特定属性”（如“用户的座位偏好”）；
- 如果是“记忆集合”，检索就复杂

# 记忆检索（接上文）
要搞定“哪些记忆该用、啥时候用”，得先看记忆是咋整理的：要是像“用户档案”那样整得规规矩矩，直接查全量档案或者某一项信息就行；可要是一堆零散的记忆，检索就复杂多了——得从一大堆没咋规整的信息里，找出跟当前对话最相关、最有用的内容，这才是真正的难点。而且还得在保证速度的前提下，挑出“真有用”的记忆，要是给模型塞了没用的记忆，反而会让回复变糟；但要是刚好找到关键信息，就能让交互一下子变智能。

高级点的记忆系统，不只会简单搜一搜，还会从好几个维度给记忆打分，挑出最合适的：
- **相关性（语义相似度）**：这个记忆跟当前对话在意思上贴得多近？
- **新鲜度（时间维度）**：这个记忆是啥时候创建的？越新的往往越有用。
- **重要性（关键程度）**：这个记忆本身有多重要？跟相关性不一样，“重要性”一般在生成记忆的时候就定下来了。

很多人容易犯的错就是“只看语义相似度”。有时候看着意思像，但其实是老早以前的、无关紧要的记忆，根本帮不上忙。所以最好的办法是“多维度结合”，把这三个维度的分数揉在一起算。

要是对准确性要求特别高，还能搞点进阶操作，比如改写查询词、重新排序，或者用专门的检索工具。但这些操作都特费劲儿，还会拖慢速度，不适合大部分实时场景。要是实在需要这些复杂操作，而且记忆不会很快过期，那可以整个“缓存”——把检索结果暂时存起来，下次再查一样的内容，就不用再费时间算了。

比如“改写查询词”，可以让大模型把用户模糊的问题改得更精准，或者把一个问题拆成好几个相关的小问题，覆盖更多角度。这样能让搜索结果更准，但开头得多调用一次大模型，会慢一点。

“重新排序”就是先通过相似度搜索，找出一大波候选记忆（比如前50个），再让大模型重新评估、排序这一小批，最后挑出更准的。

还有种方法是“微调专门的检索工具”，但这得有标注好的数据，成本也高，一般用不上。

说到底，最好的检索方法，其实是从“生成记忆”就开始下功夫。只要记忆库里存的都是高质量、有用的信息，不管咋检索，拿到的记忆都差不了。

## 检索时机
最后一个要定的架构问题是“啥时候检索记忆”，主要有两种方式：

### 1. 主动检索（每轮对话都加载）
每次对话一开始，就自动把记忆加载进来。这样能保证随时有上下文可用，但要是某轮对话用不上记忆，就白浪费时间了。不过好在单轮对话里，记忆是不变的，所以可以用缓存来提速，减少这种浪费。

比如用ADK框架的话，能直接用内置的“预加载记忆工具”，或者自己写个回调函数来实现：
```python
# 方法1：用内置的PreloadMemoryTool，每轮都通过相似度搜索拉取记忆
agent = LlmAgent(
    tools=[adk.tools.preload_memory_tool.PreloadMemoryTool()]
) 

# 方法2：自己写回调函数，更灵活地控制怎么拉取记忆
def retrieve_memories_callback(callback_context, llm_request):
    # 拿到当前用户ID和应用名
    user_id = callback_context._invocation_context.user_id
    app_name = callback_context._invocation_context.app_name
    
    # 调用API拉取记忆
    response = client.agent_engines.memories.retrieve(
        name="projects/你的项目ID/locations/你的区域/reasoningEngines/你的引擎名", 
        scope={
            "user_id": user_id, 
            "app_name": app_name
        }
    )
    
    # 把记忆整理成列表
    memories = [f"* {memory.memory.fact}" for memory in list(response)]
    # 要是没记忆，就不往系统指令里加了
    if not memories:
        return
    
    # 把记忆拼到系统指令里
    llm_request.config.system_instruction += "\n以下是你掌握的用户信息：\n"
    llm_request.config.system_instruction += "\n".join(memories) 

# 创建智能体，把回调函数加上
agent = LlmAgent(
    before_model_callback=retrieve_memories_callback,
)
```

### 2. 被动检索（“记忆即工具”）
给智能体整个“查记忆”的工具（比如叫`load_memory`），让它自己判断啥时候需要查。这种方式更高效、更靠谱，但得多调用一次大模型，会慢一点、花点钱；不过好处是“只在需要的时候查”，不会平白浪费时间。另外，智能体可能不知道有没有相关记忆，这时候可以在工具说明里告诉它“有哪些类型的记忆”，比如“存了用户喜欢的食物这类信息”，帮它做判断。

```python
# 方法1：用内置的LoadMemory工具
agent = LlmAgent(
    tools=[adk.tools.load_memory_tool.LoadMemoryTool()],
)

# 方法2：自己写工具，说明清楚有哪些记忆可以查
def load_memory(query: str, tool_context: ToolContext):
    """帮用户拉取记忆。
    目前存的用户信息包括：
    * 用户偏好，比如喜欢的食物
    """
    # 通过相似度搜索拉取记忆
    response = tool_context.search_memory(query)
    return response.memories

# 创建智能体，把自定义工具加上
agent = LlmAgent(
    tools=[load_memory],
)
```

# 基于记忆的推理
找到相关记忆后，最后一步就是“怎么把记忆塞到大模型的上下文窗口里”——这步特别关键，记忆放哪儿、怎么放，会严重影响大模型的推理，还会关系到成本和回复质量。

一般有两种主要方式：把记忆贴在系统指令后面，或者塞到对话历史里。实际用的时候，往往是“两种结合”：系统指令里放那些稳定的、全局通用的记忆（比如用户档案），保证每次都有；对话历史里放那些临时的、只跟当前对话相关的记忆（比如之前聊过的某个话题细节），灵活应对当下需求。

## 系统指令中的记忆
最简单的办法就是把记忆追加到系统指令里。这样能保持对话历史干净，直接把记忆当成“基础上下文”，跟系统指令拼在一起。比如用Jinja模板动态加记忆：

```python
from jinja2 import Template

# 写个模板，把系统指令和记忆拼起来
template = Template("""
{{ system_instructions }}
<记忆内容>
以下是关于当前用户的信息：
{% for retrieved_memory in data %}* {{ retrieved_memory.memory.fact }}
{% endfor %}
</记忆内容>
""")

# 把实际的系统指令和记忆填进模板
prompt = template.render(
    system_instructions=system_instructions,
    data=retrieved_memories
)
```

这种方式有三个好处：记忆的“权重”高，大模型会更重视；能把记忆和对话内容清楚分开；特别适合用户档案这种稳定的信息。但也有缺点——可能会“过度影响”大模型，比如不管聊啥，它都硬要往记忆上靠，哪怕不相关。

另外还有几个限制：首先，得要智能体框架支持“每调用一次大模型就动态改系统指令”，不是所有框架都有这功能；其次，跟“记忆即工具”不兼容——因为得先确定系统指令，大模型才能判断要不要调用查记忆的工具；最后，没法处理非文本记忆，大部分大模型的系统指令只认文字，图片、音频这类内容塞不进去。

## 对话历史中的记忆
这种方式是把记忆直接插进逐轮对话里，可以放在整个对话历史前面，也可以放在最新的用户提问前面。

但问题也很明显：会让对话变乱，增加token消耗；要是记忆不相关，还会让大模型迷糊。最大的风险是“对话注入”——大模型可能会误以为记忆是“之前实际说过的话”。而且插记忆的时候，得注意“视角”，比如要是用“用户”的角色，那记忆就得用第一人称写，比如“我喜欢靠窗的座位”，而不是“用户喜欢靠窗的座位”。

还有种特殊情况：通过工具调用拉取记忆。这时候记忆会作为“工具输出”，直接出现在对话里。

```python
def load_memory(query: str, tool_context: ToolContext):
    """把记忆加载到对话历史里"""
    # 调用工具拉取记忆
    response = tool_context.search_memory(query)
    return response.memories

# 创建智能体，添加这个工具
agent = LlmAgent(
    tools=[load_memory],
)
```

## 过程性记忆
前面咱们聊的基本都是“陈述性记忆”（也就是“知道是什么”），这也跟现在市面上大部分记忆工具的定位一致——它们擅长提取、存储“事实、历史、用户数据”这类信息。

但这些工具没法处理“过程性记忆”（也就是“知道怎么做”）——这种记忆是用来优化智能体的工作流程和推理方式的。存储“怎么做”不是“找信息”的问题，而是“怎么帮智能体更好地思考”的问题。要管好这种记忆，得有一套完全独立的、专门的流程，不过整体框架跟陈述性记忆差不多：

1. **提取**：得用专门的提示词，从成功的交互里提炼出“可复用的步骤”（比如“订机票的流程”），而不只是抓个事实。
2. **整合**：陈述性记忆的整合是“合并相关事实”，但过程性记忆是“优化流程本身”——比如把新的好方法跟现有的“最佳步骤”结合，修补老流程里的漏洞，删掉没用的步骤。
3. **检索**：目的不是“找数据回答问题”，而是“找一套步骤指导智能体做复杂任务”，所以过程性记忆的格式可能跟陈述性记忆不一样。

这种“智能体自己优化流程”的能力，很容易让人想到“微调”（比如基于人类反馈的强化学习RLHF）。但两者差别很大：微调是慢节奏的离线训练，会改模型的权重；而过程性记忆是“实时优化”——直接把正确的“步骤指南”塞到提示词里，通过“上下文学习”帮智能体干活，不用微调。

# 测试与评估
搞出带记忆功能的智能体后，得通过全面的测试来验证它的表现。评估得从好几个层面来：智能体是不是记了该记的（记忆质量）、要用的时候能不能找到（检索效果）、用了记忆之后能不能更好地完成任务（任务成功率）。学术界喜欢搞可重复的基准测试，但工业界更关注“记忆对实际生产环境中智能体的性能和可用性有啥影响”。

## 1. 记忆生成质量指标
看记忆本身好不好，核心是“智能体有没有记对东西”。一般会把智能体生成的记忆，跟人工整理的“标准答案记忆”对比：
- **精确率**：智能体生成的所有记忆里，有多少是准确、相关的？精确率高，能避免记忆库被没用的信息塞满。
- **召回率**：从对话里该记的关键信息中，智能体抓住了多少？召回率高，能保证不遗漏重要内容。
- **F1分数**：精确率和召回率的调和平均数，用来综合衡量记忆质量。

## 2. 记忆检索性能指标
看智能体能不能在需要的时候找到正确的记忆：
- **Recall@K（前K召回率）**：需要某段记忆时，它能不能出现在前K个检索结果里？这是判断检索系统准不准的核心指标。
- **延迟**：检索是用户交互的“关键路径”，整个过程必须快（比如200毫秒以内），不然会影响用户体验。

## 3. 端到端任务成功率指标
这是最终的测试——“记忆到底能不能帮智能体把活干得更好？”。一般会让智能体用记忆完成任务，再让另一个大模型当“裁判”，把智能体的输出和“标准答案”对比，判断准不准，以此衡量记忆系统的贡献。

评估不是搞一次就完了，而是“持续优化的循环”：先定个基准，分析哪里不行，调整系统（比如改提示词、优化检索算法），再重新评估看效果。

除了质量，生产环境还得关注性能：每个评估环节都要测算法的延迟，以及能不能扛住高并发。检索是“关键路径”，得保证秒级以内的速度；生成和整合虽然一般是异步处理，但也得有足够的吞吐量，能跟上用户需求。说到底，好的记忆系统既要“聪明”，又要“高效、靠谱”，能应对真实场景。

# 记忆的生产环境考量
把带记忆的智能体从原型搞到生产环境，除了性能，还得关注企业级的架构问题——可扩展性、容错能力、安全性都得达标。生产级的系统不光要“智能”，还得“稳”。

要保证用户体验不被“生成记忆”这种费劲儿的操作拖慢，关键是把“记忆处理”和“主应用逻辑”拆开。虽然这是个事件驱动的模式，但一般不用自己搭消息队列，直接调用专门的记忆服务API就行，而且要“非阻塞”——具体流程是这样的：

1. **智能体推数据**：比如对话结束后，智能体调用记忆服务的API，把原始数据（比如对话记录）“推”过去，不用等结果。
2. **记忆服务后台处理**：记忆服务马上确认“收到了”，然后把生成任务放进自己的队列里，异步处理——比如调用大模型提取、整合记忆。有时候还会等用户隔一会儿不操作了再处理，避免频繁触发。
3. **记忆持久化存储**：服务把最终的记忆（可能是新的，也可能是更新旧的）存到专门的数据库里。要是用托管的记忆服务，存储都是现成的。
4. **智能体拉取记忆**：下次用户交互需要上下文时，智能体直接查这个记忆库就行。

这种“基于服务的非阻塞方式”有个大好处：就算记忆处理出问题或者变慢，也不会影响用户用应用，系统更抗造。另外，还能选“实时生成”（适合保证对话新鲜度）或者“离线批量处理”（适合用历史数据初始化系统）。

随着应用用户变多，记忆系统得能扛住高频请求，还不能出错。比如多个请求同时要改同一个记忆，得防止“死锁”或者“竞争冲突”——可以用数据库的事务操作，或者“乐观锁”来解决，但这可能会导致请求排队。所以还得整个靠谱的消息队列，缓冲大量请求，别让记忆服务被压垮。

记忆服务还得能应对“临时错误”：比如调用大模型失败了，得有“重试机制”（比如失败后等一会儿再试，间隔越变越长），要是一直失败，就放到“死信队列”里，之后分析原因。

要是应用是全球用的，记忆服务的数据库得支持“多区域同步”，保证速度快、不宕机。不能在客户端同步数据，因为整合记忆需要“数据一致”，不然会冲突。所以记忆系统得自己处理同步，对外只给开发者一个“统一的数据库接口”，但背后要保证全球数据一致。

像“Agent Engine Memory Bank”这种托管记忆服务，会帮你搞定这些生产级的问题，你只用专注于智能体的核心逻辑就行。

# 隐私与安全风险
记忆里都是用户数据，所以隐私和安全控制必须严。可以这么理解：记忆系统就像公司里的“保密档案室”，管理员（对应系统）的活儿就是“存有用的信息，同时护好安全”。

这个档案室的“铁规矩”是“数据隔离”——就像管理员不会把不同部门的机密文件混在一起，记忆也得按用户或者租户严格分开。给A用户服务的智能体，绝对不能拿到B用户的记忆，这得靠严格的访问控制列表（ACL）来实现。另外，用户得能自己控制数据，比如可以关了记忆生成，或者要求删除所有自己的记忆。

管理员存文件前，还得做两件关键的安全操作：第一，把敏感的个人信息（PII）删掉，比如手机号、邮箱，这样既存了有用的信息，又不会有泄露风险；第二，得识别出假的、骗人的信息，防止有人故意塞坏数据（也就是“记忆投毒”）。同理，系统在把信息存成长期记忆前，也得验证、清理，比如用Model Armor这种工具，防止恶意用户通过提示词注入搞坏智能体的记忆。

还有个“信息泄露”的风险：要是多个用户共享一套记忆（比如过程性记忆，教智能体怎么做某事），比如把A用户的流程当例子给B用户看，得先把敏感信息完全去掉，不然会跨用户泄露数据。

# 结论
这份白皮书聊了“上下文工程”这个领域，重点讲了它的两个核心部分：对话会话（Sessions）和记忆（Memory）。从简单的一轮对话，到能长期用、能落地的智能信息，全靠上下文工程——它要把对话历史、记忆、外部知识这些所有需要的信息，动态整合成大模型能用上的上下文。而这一切，都依赖“会话”和“记忆”这两个既独立又关联的系统。

会话管的是“当下”——它是单次对话的“临时容器”，得快、得安全：访问速度要快，用户才觉得流畅；数据要严格隔离，不能串了。为了防止上下文窗口不够用、拖慢速度，还得用“按token截断”“递归总结”这些方法，压缩会话里的内容。另外，安全方面，会话数据存之前，必须把敏感个人信息（PII）删掉。

记忆则是“长期个性化”的核心，也是跨会话持久化信息的关键。它跟RAG不一样——RAG让智能体成了“事实专家”，而记忆让智能体成了“用户专家”。记忆是个“大模型驱动的主动ETL流程”，要做提取、整合、检索这三件事，从对话里提炼最重要的信息：提取阶段把关键信息抓出来，整合成记忆；整合阶段把新记忆跟旧的合并，解决冲突、删掉重复的，保证记忆库不乱；而且为了让用户觉得快，生成记忆得在智能体回复完用户后，在后台异步处理。只要跟踪好记忆的来源，做好“防记忆投毒”这类安全措施，开发者就能做出真正“懂用户、会成长”的智能助手。

# 尾注
1. https://cloud.google.com/use-cases/retrieval-augmented-generation?hl=en（检索增强生成相关文档）
2. https://arxiv.org/abs/2301.00234（上下文学习相关论文）
3. https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/sessions/overview（Agent Engine会话服务文档）
4. https://langchain-ai.github.io/langgraph/concepts/multi_agent/#message-passing-between-agents（多智能体消息传递相关文档）
5. https://google.github.io/adk-docs/agents/multi-agents/（ADK多智能体相关文档）
6. https://google.github.io/adk-docs/agents/multi-agents/#c-explicit-invocation-agenttool（“智能体即工具”调用相关文档）
7. https://agent2agent.info/docs/concepts/message/（智能体间通信协议相关文档）
8. https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/（智能体互操作性相关博客）
9. https://cloud.google.com/security-command-center/docs/model-armor-overview（Model Armor工具相关文档）
10. https://ai.google.dev/gemini-api/docs/long-context#long-context-limitations（Gemini长上下文限制相关文档）
11. https://huggingface.co/blog/Kseniase/memory（记忆相关博客）
12. https://langchain-ai.github.io/langgraph/concepts/memory/#semantic-memory（语义记忆相关文档）
13. https://langchain-ai.github.io/langgraph/concepts/memory/#semantic-memory（语义记忆相关文档，同12）
14. https://arxiv.org/pdf/2412.15266（原子事实相关论文）
15. https://arxiv.org/pdf/2412.15266（知识三元组相关论文，同14）
16. https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#sample-requests-text-gen-multimodal-prompt（多模态生成请求示例相关文档）
17. https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/memory-bank/generate-memories（Agent Engine记忆生成相关文档）
18. https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output（控制多模态生成结果相关文档）
19. https://cloud.google.com/agent-builder/agent-engine/memory-bank/set-up#memory-bank-config（Agent Engine记忆库配置相关文档）
20. https://arxiv.org/html/2504.19413v1（滚动总结相关论文）
21. https://google.github.io/adk-docs/tools/#how-agents-use-tools（ADK工具使用相关文档）
22. https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/memory-bank/generate-memories#consolidate-pre-extracted-memories（记忆整合相关文档）
23. https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/memory-bank/generate-memories#background-memory-generation（后台生成记忆相关文档）
24. https://arxiv.org/pdf/2503.08026（记忆重新排序相关论文）
25. https://google.github.io/adk-docs/callbacks/（ADK回调函数相关文档）
26. https://arxiv.org/html/2508.06433v2（过程性记忆相关论文）
27. https://cloud.google.com/blog/products/ai-machine-learning/rlhf-on-google-cloud（Google Cloud上的RLHF相关博客）
28. https://arxiv.org/pdf/2503.03704（记忆投毒相关论文）
29. https://cloud.google.com/security-command-center/docs/model-armor-overview（Model Armor工具相关文档，同9）
30. https://cloud.google.com/architecture/choose-design-pattern-agentic-ai-system（智能体系统设计模式选择相关文档）